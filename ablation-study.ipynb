{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12310440,"sourceType":"datasetVersion","datasetId":7759473},{"sourceId":12314751,"sourceType":"datasetVersion","datasetId":7762253},{"sourceId":12315834,"sourceType":"datasetVersion","datasetId":7762858}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ablation Study","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential, Model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport glob\n\nimport cv2\nfrom PIL import Image\nfrom keras.layers import Input, Dense, BatchNormalization, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Flatten, MaxPooling2D, Dropout, Reshape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the path to the dataset\nPATH1 = '/kaggle/input/dfuc-parta-resized/Abnormal'\n\n# List all images in the folder\nimages = os.listdir(PATH1)\nprint(f'There are {len(images)} pictures of Abnormal.')\n\n# Create a grid of 3x3 images\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n\nfor indx, axis in enumerate(axes.flatten()):\n    # Randomly select an image\n    rnd_indx = np.random.randint(0, len(images))\n    img_path = os.path.join(PATH1, images[rnd_indx])  # Construct the correct file path\n    img = plt.imread(img_path)  # Read the image\n    imgplot = axis.imshow(img)  # Display the image\n    axis.set_title(images[rnd_indx])  # Set the title to the image filename\n    axis.set_axis_off()  # Turn off axis ticks and labels\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the path to the dataset\nPATH1 = '/kaggle/input/dfuc-parta-resized/Normal'\n\n# List all images in the folder\nimages = os.listdir(PATH1)\nprint(f'There are {len(images)} pictures of Normal.')\n\n# Create a grid of 3x3 images\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n\nfor indx, axis in enumerate(axes.flatten()):\n    # Randomly select an image\n    rnd_indx = np.random.randint(0, len(images))\n    img_path = os.path.join(PATH1, images[rnd_indx])  # Construct the correct file path\n    img = plt.imread(img_path)  # Read the image\n    imgplot = axis.imshow(img)  # Display the image\n    axis.set_title(images[rnd_indx])  # Set the title to the image filename\n    axis.set_axis_off()  # Turn off axis ticks and labels\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 1:  Without Data Augmentation","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/dfuc-parta-resized'\n\nBATCH_SIZE = 32\nIMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\nNUM_CLASSES = 2\nEPOCHS = 200","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\n\n# Path to your dataset (containing two subfolders: Normal/ and Abnormal/)\ndata_dir = \"/kaggle/input/dfuc-parta-resized\"  # Change to your dataset path\n\n# Load dataset without any transform\ntrain_data = ImageFolder(root=data_dir)\n\n# Get class names (folder names)\nclass_names = train_data.classes\n\n# Randomly select 10 image indices\nindices = random.sample(range(len(train_data)), 10)\n\n# Plot\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor i, idx in enumerate(indices):\n    path, label = train_data.samples[idx]  # This gives image path and label\n    img = Image.open(path).convert(\"RGB\")  # Open and ensure 3 channels\n\n    axes[i].imshow(img)\n    axes[i].set_title(f\"Label: {class_names[label]}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print class name details\nprint(\"Class Names:\", class_names)\nprint(\"Index of 'Abnormal':\", class_names.index(\"Abnormal\"))\nprint(\"Index of 'Normal':\", class_names.index(\"Normal\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weights_init(m):\n    \"\"\"\n    Takes as input a neural network m that will initialize all its weights.\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)  #Stabilizes early training, avoids large weights\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)   #Keeps initial scaling/shift minimal\n        m.bias.data.fill_(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import EfficientNetB0, ResNet50\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import callbacks\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Constants\nTRAIN_PATH = '/kaggle/input/dfuc-parta-resized'\n\nBATCH_SIZE = 32\nIMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\nNUM_CLASSES = 2\nEPOCHS = 200","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/dfuc-parta-resized'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/dfuc-parta-resized\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim                      # Total embedding dimension\n        self.num_heads = num_heads                      # Number of attention heads\n        self.head_dim = embed_dim // num_heads          # Dimension per attention head\n        \n        # Ensure embedding dimension is divisible by number of heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        # Dense layers to project inputs into queries, keys, and values\n        self.query_dense = layers.Dense(embed_dim)  \n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        \n        # Dense layer to combine multiple attention heads output\n        self.combine_heads = layers.Dense(embed_dim)\n        \n    def attention(self, query, key, value):\n        # Calculate scaled dot-product attention scores\n        score = tf.matmul(query, key, transpose_b=True)   # Compute dot product of query and key^T\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Dimensionality for scaling\n        scaled_score = score / tf.math.sqrt(dim_key)      # Scale scores to stabilize gradients\n        weights = tf.nn.softmax(scaled_score, axis=-1)    # Apply softmax to get attention weights\n        output = tf.matmul(weights, value)                 # Weighted sum of values based on attention weights\n        return output, weights\n    \n    def separate_heads(self, x, batch_size):\n        # Reshape the input tensor to split into multiple attention heads\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim)) \n        # Transpose to shape: (batch_size, num_heads, sequence_length, head_dim)\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        \n        # Flatten spatial dimensions (height and width) into one sequence dimension\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))  # shape: (batch, seq_len, channels)\n        \n        # Create queries, keys, and values projections\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        \n        # Separate each into multiple heads\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        \n        # Compute attention output and weights\n        attention_output, attention_weights = self.attention(query, key, value)\n        \n        # Transpose back to (batch_size, seq_len, embed_dim)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        \n        # Concatenate attention heads\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        \n        # Final dense layer to combine heads into output\n        output = self.combine_heads(concat_attention)\n        \n        # Reshape output back to spatial format (batch_size, height, width, embed_dim)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        \n        return output\n\n# Spatial Attention Block\n#SpatialAttentionBlock is designed to perform spatial attention—that is, it learns where in the image the model should \"pay more attention\" by assigning weights to each spatial location\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        # 1x1 convolutions for query, key, and value feature generation\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)  # Final convolution after attention\n        self.softmax = layers.Softmax(axis=-1)    # Softmax for attention weights\n        \n    def call(self, inputs):\n        # Get dynamic shapes for batch size, height, width, channels\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        \n        # Generate query, key, and value feature maps from input\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        \n        # Reshape to (batch_size, sequence_length, filters) for matrix multiplication\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n        \n        # Compute attention scores by dot product of query and key transpose\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))  # Scale scores\n        \n        # Softmax to obtain attention weights\n        attention_weights = self.softmax(attention_scores)\n        \n        # Weighted sum of values based on attention weights\n        attended_features = tf.matmul(attention_weights, value)\n        \n        # Reshape attended features back to spatial format\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        \n        # Final convolution to refine features\n        output = self.conv_out(attended_features)\n        \n        # Add residual connection if input channels match filter count\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        \n        return output\n\n# Channel Attention Block\n#In Channel Attention, the model learns to weigh the importance of each channel in a feature map.\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        \n        # Global average and max pooling layers\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        \n        # Dense layers to learn channel-wise attention weights\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Average pooling branch: captures average channel information\n        avg_pool = self.global_avg_pool(inputs)                     # Shape: (batch_size, channels)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)   # Reshape to (batch_size, 1, 1, channels)\n        avg_pool = self.dense1(avg_pool)                             # Reduce dimension and non-linearity\n        avg_pool = self.dense2(avg_pool)                             # Channel attention weights via sigmoid\n        \n        # Max pooling branch: captures prominent channel features\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n        \n        # Combine attention weights from both branches\n        attention = avg_pool + max_pool\n        \n        # Multiply input features by attention weights (channel-wise scaling)\n        return inputs * attention\n\n# ResNet-like Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_attention = use_attention\n        \n        # Two convolutional layers with batch normalization\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Attention blocks (spatial and channel) if enabled\n        if use_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n            self.channel_attention = ChannelAttentionBlock(filters)\n        \n        self.relu = layers.ReLU()\n        \n    def call(self, inputs):\n        # First conv layer with activation and batch norm\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        \n        # Second conv layer with batch norm (no activation yet)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        # Apply attention blocks if enabled\n        if self.use_attention:\n            x = self.spatial_attention(x)\n            x = self.channel_attention(x)\n        \n        # Residual skip connection if input and output have same channel dimension\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        \n        # Final activation after residual addition\n        return self.relu(x)\n\n# CNN Model with Self-Attention\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    \"\"\"\n    Build CNN model with self-attention mechanisms\n    \n    Args:\n        IMAGE_WIDTH: Input image width\n        IMAGE_HEIGHT: Input image height  \n        NUM_CLASSES: Number of output classes\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    \n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))  # Input layer with image shape\n    \n    # Normalize pixel values to [0,1]\n    x = layers.Rescaling(1./255)(inputs)\n    \n    # Initial convolutional layer with large kernel and stride for downsampling\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    # Stage 1: Basic convolutions with batch norm\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Stage 2: Residual blocks with attention + downsampling\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    \n    # Stage 3: Increased filter count and residual attention blocks + downsampling\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    \n    # Stage 4: High-level features + self-attention + downsampling\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Apply self-attention over feature map\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)\n    \n    # Additional residual attention blocks after self-attention\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    \n    # Global average pooling to reduce spatial dimensions\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Dropout for regularization to prevent overfitting\n    x = layers.Dropout(0.3)(x)\n    \n    # Dense layer with ReLU activation for learning complex features\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer:\n    # For binary classification, use sigmoid activation and binary crossentropy loss\n    # For multi-class, use softmax activation and sparse categorical crossentropy\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n    \n    # Create the Keras model instance\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    \n    # Compile model with Adam optimizer, selected loss, and common classification metrics\n    optimizer = optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=[\n            'accuracy',                            # Classification accuracy\n            metrics.Precision(name='precision'), # Precision metric\n            metrics.Recall(name='recall'),       # Recall metric\n            metrics.AUC(name='auc')               # Area under ROC curve\n        ]\n    )\n\n    return model\n\n\n# Usage example\nif __name__ == \"__main__\":\n    # Define input image dimensions and number of classes\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n    \n    print(\"Building CNN with Self-Attention model...\")\n    \n    # Build the model\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    \n    # Print the model summary showing layers and parameters\n    model.summary()\n    \n    # Generate a dummy input tensor (batch size 1, 224x224 RGB image)\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    \n    # Run the model on dummy data to test forward pass\n    output = model(dummy_input)\n    \n    # Print output tensor shape\n    print(f\"\\nModel output shape: {output.shape}\")\n    \n    # Print actual output values (probabilities or logits)\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 2: Data Augmentation using CGAN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import EfficientNetB0, ResNet50\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import callbacks\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-cgan/resized_dataset_CGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-cgan/resized_dataset_CGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim                      # Total embedding dimension\n        self.num_heads = num_heads                      # Number of attention heads\n        self.head_dim = embed_dim // num_heads          # Dimension per attention head\n        \n        # Ensure embedding dimension is divisible by number of heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        # Dense layers to project inputs into queries, keys, and values\n        self.query_dense = layers.Dense(embed_dim)  \n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        \n        # Dense layer to combine multiple attention heads output\n        self.combine_heads = layers.Dense(embed_dim)\n        \n    def attention(self, query, key, value):\n        # Calculate scaled dot-product attention scores\n        score = tf.matmul(query, key, transpose_b=True)   # Compute dot product of query and key^T\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Dimensionality for scaling\n        scaled_score = score / tf.math.sqrt(dim_key)      # Scale scores to stabilize gradients\n        weights = tf.nn.softmax(scaled_score, axis=-1)    # Apply softmax to get attention weights\n        output = tf.matmul(weights, value)                 # Weighted sum of values based on attention weights\n        return output, weights\n    \n    def separate_heads(self, x, batch_size):\n        # Reshape the input tensor to split into multiple attention heads\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim)) \n        # Transpose to shape: (batch_size, num_heads, sequence_length, head_dim)\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        \n        # Flatten spatial dimensions (height and width) into one sequence dimension\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))  # shape: (batch, seq_len, channels)\n        \n        # Create queries, keys, and values projections\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        \n        # Separate each into multiple heads\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        \n        # Compute attention output and weights\n        attention_output, attention_weights = self.attention(query, key, value)\n        \n        # Transpose back to (batch_size, seq_len, embed_dim)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        \n        # Concatenate attention heads\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        \n        # Final dense layer to combine heads into output\n        output = self.combine_heads(concat_attention)\n        \n        # Reshape output back to spatial format (batch_size, height, width, embed_dim)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        \n        return output\n\n# Spatial Attention Block\n#SpatialAttentionBlock is designed to perform spatial attention—that is, it learns where in the image the model should \"pay more attention\" by assigning weights to each spatial location\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        # 1x1 convolutions for query, key, and value feature generation\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)  # Final convolution after attention\n        self.softmax = layers.Softmax(axis=-1)    # Softmax for attention weights\n        \n    def call(self, inputs):\n        # Get dynamic shapes for batch size, height, width, channels\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        \n        # Generate query, key, and value feature maps from input\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        \n        # Reshape to (batch_size, sequence_length, filters) for matrix multiplication\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n        \n        # Compute attention scores by dot product of query and key transpose\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))  # Scale scores\n        \n        # Softmax to obtain attention weights\n        attention_weights = self.softmax(attention_scores)\n        \n        # Weighted sum of values based on attention weights\n        attended_features = tf.matmul(attention_weights, value)\n        \n        # Reshape attended features back to spatial format\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        \n        # Final convolution to refine features\n        output = self.conv_out(attended_features)\n        \n        # Add residual connection if input channels match filter count\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        \n        return output\n\n# Channel Attention Block\n#In Channel Attention, the model learns to weigh the importance of each channel in a feature map.\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        \n        # Global average and max pooling layers\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        \n        # Dense layers to learn channel-wise attention weights\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Average pooling branch: captures average channel information\n        avg_pool = self.global_avg_pool(inputs)                     # Shape: (batch_size, channels)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)   # Reshape to (batch_size, 1, 1, channels)\n        avg_pool = self.dense1(avg_pool)                             # Reduce dimension and non-linearity\n        avg_pool = self.dense2(avg_pool)                             # Channel attention weights via sigmoid\n        \n        # Max pooling branch: captures prominent channel features\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n        \n        # Combine attention weights from both branches\n        attention = avg_pool + max_pool\n        \n        # Multiply input features by attention weights (channel-wise scaling)\n        return inputs * attention\n\n# ResNet-like Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_attention = use_attention\n        \n        # Two convolutional layers with batch normalization\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Attention blocks (spatial and channel) if enabled\n        if use_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n            self.channel_attention = ChannelAttentionBlock(filters)\n        \n        self.relu = layers.ReLU()\n        \n    def call(self, inputs):\n        # First conv layer with activation and batch norm\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        \n        # Second conv layer with batch norm (no activation yet)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        # Apply attention blocks if enabled\n        if self.use_attention:\n            x = self.spatial_attention(x)\n            x = self.channel_attention(x)\n        \n        # Residual skip connection if input and output have same channel dimension\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        \n        # Final activation after residual addition\n        return self.relu(x)\n\n# CNN Model with Self-Attention\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    \"\"\"\n    Build CNN model with self-attention mechanisms\n    \n    Args:\n        IMAGE_WIDTH: Input image width\n        IMAGE_HEIGHT: Input image height  \n        NUM_CLASSES: Number of output classes\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    \n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))  # Input layer with image shape\n    \n    # Normalize pixel values to [0,1]\n    x = layers.Rescaling(1./255)(inputs)\n    \n    # Initial convolutional layer with large kernel and stride for downsampling\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    # Stage 1: Basic convolutions with batch norm\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Stage 2: Residual blocks with attention + downsampling\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    \n    # Stage 3: Increased filter count and residual attention blocks + downsampling\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    \n    # Stage 4: High-level features + self-attention + downsampling\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Apply self-attention over feature map\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)\n    \n    # Additional residual attention blocks after self-attention\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    \n    # Global average pooling to reduce spatial dimensions\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Dropout for regularization to prevent overfitting\n    x = layers.Dropout(0.3)(x)\n    \n    # Dense layer with ReLU activation for learning complex features\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer:\n    # For binary classification, use sigmoid activation and binary crossentropy loss\n    # For multi-class, use softmax activation and sparse categorical crossentropy\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n    \n    # Create the Keras model instance\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    \n    # Compile model with Adam optimizer, selected loss, and common classification metrics\n    optimizer = optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=[\n            'accuracy',                            # Classification accuracy\n            metrics.Precision(name='precision'), # Precision metric\n            metrics.Recall(name='recall'),       # Recall metric\n            metrics.AUC(name='auc')               # Area under ROC curve\n        ]\n    )\n\n    return model\n\n\n# Usage example\nif __name__ == \"__main__\":\n    # Define input image dimensions and number of classes\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n    \n    print(\"Building CNN with Self-Attention model...\")\n    \n    # Build the model\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    \n    # Print the model summary showing layers and parameters\n    model.summary()\n    \n    # Generate a dummy input tensor (batch size 1, 224x224 RGB image)\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    \n    # Run the model on dummy data to test forward pass\n    output = model(dummy_input)\n    \n    # Print output tensor shape\n    print(f\"\\nModel output shape: {output.shape}\")\n    \n    # Print actual output values (probabilities or logits)\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 3: Data Augmentation using Enhanced CGAN","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim                      # Total embedding dimension\n        self.num_heads = num_heads                      # Number of attention heads\n        self.head_dim = embed_dim // num_heads          # Dimension per attention head\n        \n        # Ensure embedding dimension is divisible by number of heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        # Dense layers to project inputs into queries, keys, and values\n        self.query_dense = layers.Dense(embed_dim)  \n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        \n        # Dense layer to combine multiple attention heads output\n        self.combine_heads = layers.Dense(embed_dim)\n        \n    def attention(self, query, key, value):\n        # Calculate scaled dot-product attention scores\n        score = tf.matmul(query, key, transpose_b=True)   # Compute dot product of query and key^T\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Dimensionality for scaling\n        scaled_score = score / tf.math.sqrt(dim_key)      # Scale scores to stabilize gradients\n        weights = tf.nn.softmax(scaled_score, axis=-1)    # Apply softmax to get attention weights\n        output = tf.matmul(weights, value)                 # Weighted sum of values based on attention weights\n        return output, weights\n    \n    def separate_heads(self, x, batch_size):\n        # Reshape the input tensor to split into multiple attention heads\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim)) \n        # Transpose to shape: (batch_size, num_heads, sequence_length, head_dim)\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        \n        # Flatten spatial dimensions (height and width) into one sequence dimension\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))  # shape: (batch, seq_len, channels)\n        \n        # Create queries, keys, and values projections\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        \n        # Separate each into multiple heads\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        \n        # Compute attention output and weights\n        attention_output, attention_weights = self.attention(query, key, value)\n        \n        # Transpose back to (batch_size, seq_len, embed_dim)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        \n        # Concatenate attention heads\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        \n        # Final dense layer to combine heads into output\n        output = self.combine_heads(concat_attention)\n        \n        # Reshape output back to spatial format (batch_size, height, width, embed_dim)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        \n        return output\n\n# Spatial Attention Block\n#SpatialAttentionBlock is designed to perform spatial attention—that is, it learns where in the image the model should \"pay more attention\" by assigning weights to each spatial location\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        # 1x1 convolutions for query, key, and value feature generation\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)  # Final convolution after attention\n        self.softmax = layers.Softmax(axis=-1)    # Softmax for attention weights\n        \n    def call(self, inputs):\n        # Get dynamic shapes for batch size, height, width, channels\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        \n        # Generate query, key, and value feature maps from input\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        \n        # Reshape to (batch_size, sequence_length, filters) for matrix multiplication\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n        \n        # Compute attention scores by dot product of query and key transpose\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))  # Scale scores\n        \n        # Softmax to obtain attention weights\n        attention_weights = self.softmax(attention_scores)\n        \n        # Weighted sum of values based on attention weights\n        attended_features = tf.matmul(attention_weights, value)\n        \n        # Reshape attended features back to spatial format\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        \n        # Final convolution to refine features\n        output = self.conv_out(attended_features)\n        \n        # Add residual connection if input channels match filter count\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        \n        return output\n\n# Channel Attention Block\n#In Channel Attention, the model learns to weigh the importance of each channel in a feature map.\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        \n        # Global average and max pooling layers\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        \n        # Dense layers to learn channel-wise attention weights\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Average pooling branch: captures average channel information\n        avg_pool = self.global_avg_pool(inputs)                     # Shape: (batch_size, channels)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)   # Reshape to (batch_size, 1, 1, channels)\n        avg_pool = self.dense1(avg_pool)                             # Reduce dimension and non-linearity\n        avg_pool = self.dense2(avg_pool)                             # Channel attention weights via sigmoid\n        \n        # Max pooling branch: captures prominent channel features\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n        \n        # Combine attention weights from both branches\n        attention = avg_pool + max_pool\n        \n        # Multiply input features by attention weights (channel-wise scaling)\n        return inputs * attention\n\n# ResNet-like Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_attention = use_attention\n        \n        # Two convolutional layers with batch normalization\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Attention blocks (spatial and channel) if enabled\n        if use_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n            self.channel_attention = ChannelAttentionBlock(filters)\n        \n        self.relu = layers.ReLU()\n        \n    def call(self, inputs):\n        # First conv layer with activation and batch norm\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        \n        # Second conv layer with batch norm (no activation yet)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        # Apply attention blocks if enabled\n        if self.use_attention:\n            x = self.spatial_attention(x)\n            x = self.channel_attention(x)\n        \n        # Residual skip connection if input and output have same channel dimension\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        \n        # Final activation after residual addition\n        return self.relu(x)\n\n# CNN Model with Self-Attention\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    \"\"\"\n    Build CNN model with self-attention mechanisms\n    \n    Args:\n        IMAGE_WIDTH: Input image width\n        IMAGE_HEIGHT: Input image height  \n        NUM_CLASSES: Number of output classes\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    \n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))  # Input layer with image shape\n    \n    # Normalize pixel values to [0,1]\n    x = layers.Rescaling(1./255)(inputs)\n    \n    # Initial convolutional layer with large kernel and stride for downsampling\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    # Stage 1: Basic convolutions with batch norm\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Stage 2: Residual blocks with attention + downsampling\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    \n    # Stage 3: Increased filter count and residual attention blocks + downsampling\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    \n    # Stage 4: High-level features + self-attention + downsampling\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Apply self-attention over feature map\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)\n    \n    # Additional residual attention blocks after self-attention\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    \n    # Global average pooling to reduce spatial dimensions\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Dropout for regularization to prevent overfitting\n    x = layers.Dropout(0.3)(x)\n    \n    # Dense layer with ReLU activation for learning complex features\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer:\n    # For binary classification, use sigmoid activation and binary crossentropy loss\n    # For multi-class, use softmax activation and sparse categorical crossentropy\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n    \n    # Create the Keras model instance\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    \n    # Compile model with Adam optimizer, selected loss, and common classification metrics\n    optimizer = optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=[\n            'accuracy',                            # Classification accuracy\n            metrics.Precision(name='precision'), # Precision metric\n            metrics.Recall(name='recall'),       # Recall metric\n            metrics.AUC(name='auc')               # Area under ROC curve\n        ]\n    )\n\n    return model\n\n\n# Usage example\nif __name__ == \"__main__\":\n    # Define input image dimensions and number of classes\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n    \n    print(\"Building CNN with Self-Attention model...\")\n    \n    # Build the model\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    \n    # Print the model summary showing layers and parameters\n    model.summary()\n    \n    # Generate a dummy input tensor (batch size 1, 224x224 RGB image)\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    \n    # Run the model on dummy data to test forward pass\n    output = model(dummy_input)\n    \n    # Print output tensor shape\n    print(f\"\\nModel output shape: {output.shape}\")\n    \n    # Print actual output values (probabilities or logits)\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 4: Data Augmentation using Enhanced CGAN,\tDropout: 0.3\tNo. of epochs: 200, No. of layers: 17\tSelf-attention: YES,\tSpatial attention: NO","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        shape = tf.shape(inputs)\n        height, width = shape[1], shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, shape[3]))\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        attention_output, _ = self.attention(query, key, value)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        output = self.combine_heads(concat_attention)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        return output\n\n# Channel Attention Block\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n\n    def call(self, inputs):\n        avg_pool = self.global_avg_pool(inputs)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)\n        avg_pool = self.dense1(avg_pool)\n        avg_pool = self.dense2(avg_pool)\n\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n\n        attention = avg_pool + max_pool\n        return inputs * attention\n\n# Residual Block with Only Channel Attention (Spatial attention disabled)\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_channel_attention=True, use_spatial_attention=False, **kwargs):  # --- MODIFIED for Ablation Case 4 ---\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_channel_attention = use_channel_attention\n        self.use_spatial_attention = use_spatial_attention\n\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if self.use_channel_attention:\n            self.channel_attention = ChannelAttentionBlock(filters)\n        # SpatialAttentionBlock is disabled per ablation case\n\n        self.relu = layers.ReLU()\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.use_channel_attention:\n            x = self.channel_attention(x)\n        # Skipping spatial attention\n\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        return self.relu(x)\n\n# CNN Model with Self-Attention and Dropout\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    x = layers.Rescaling(1./255)(inputs)\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n    # Stage 1\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Stage 2\n    x = ResidualAttentionBlock(128, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED for Ablation Case 4 ---\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n\n    # Stage 3\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n\n    # Stage 4\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n    x = layers.MaxPooling2D(2)(x)\n\n    # Self-Attention enabled\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)\n\n    # Post Self-Attention Residuals\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=False)(x)  # --- MODIFIED\n\n    # Global Pooling + Dropout + Dense\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)  # --- Ensures Dropout 0.3 as per Ablation Case 4 ---\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-4),\n        loss=loss_fn,\n        metrics=[\n            'accuracy',\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n# Usage Example\nif __name__ == \"__main__\":\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n\n    print(\"Building CNN for Ablation Case 4 (Self-Attention + No Spatial Attention)...\")\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    model.summary()\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    output = model(dummy_input)\n    print(f\"\\nModel output shape: {output.shape}\")\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 5: Data Augmentation using Enhanced CGAN,\tDropout: 0.5\tNo. of epochs: 200, No. of layers: 17\tSelf-attention: NO,\tSpatial attention: YES","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer (NOT USED IN THIS CASE)\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        shape = tf.shape(inputs)\n        height, width = shape[1], shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, shape[3]))\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        attention_output, _ = self.attention(query, key, value)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        output = self.combine_heads(concat_attention)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        return output\n\n# Spatial Attention Block (ENABLED IN CASE 5)\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))\n        attention_weights = self.softmax(attention_scores)\n        attended_features = tf.matmul(attention_weights, value)\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        output = self.conv_out(attended_features)\n\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        return output\n\n# Channel Attention Block\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n\n    def call(self, inputs):\n        avg_pool = self.global_avg_pool(inputs)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)\n        avg_pool = self.dense1(avg_pool)\n        avg_pool = self.dense2(avg_pool)\n\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n\n        attention = avg_pool + max_pool\n        return inputs * attention\n\n# Residual Block with Spatial Attention ENABLED and Self-Attention DISABLED\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_channel_attention=True, use_spatial_attention=True, **kwargs):  # --- MODIFIED for CASE 5 ---\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_channel_attention = use_channel_attention\n        self.use_spatial_attention = use_spatial_attention\n\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if self.use_channel_attention:\n            self.channel_attention = ChannelAttentionBlock(filters)\n        if self.use_spatial_attention:  # --- ENABLED ---\n            self.spatial_attention = SpatialAttentionBlock(filters)\n\n        self.relu = layers.ReLU()\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.use_spatial_attention:   # --- ENABLED in Case 5 ---\n            x = self.spatial_attention(x)\n        if self.use_channel_attention:\n            x = self.channel_attention(x)\n\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        return self.relu(x)\n\n# CNN Model Builder\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    x = layers.Rescaling(1./255)(inputs)\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n    # Stage 1\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Stage 2\n    x = ResidualAttentionBlock(128, use_channel_attention=True, use_spatial_attention=True)(x)  # --- MODIFIED\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_channel_attention=True, use_spatial_attention=True)(x)\n\n    # Stage 3\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=True)(x)\n\n    # Stage 4\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    #  Self-Attention is DISABLED in Case 5 — Skipped\n\n    # Post Stage 4 Residual Blocks\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=True)(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Dropout(0.5)(x)  # --- MODIFIED: Dropout increased to 0.5\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)  # --- MODIFIED: Dropout increased to 0.5\n\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n\n    model = Model(inputs, outputs, name=\"CNN_SpatialAttention\")\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-4),\n        loss=loss_fn,\n        metrics=[\n            'accuracy',\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n# Run the model\nif __name__ == \"__main__\":\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n\n    print(\"Building CNN for Ablation Case 5 (Spatial Attention = YES, Self-Attention = NO)...\")\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    model.summary()\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    output = model(dummy_input)\n    print(f\"\\nModel output shape: {output.shape}\")\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 6: Data Augmentation using Enhanced CGAN,\tDropout: 0.5\tNo. of epochs: 300, No. of layers: 16\tSelf-attention: YES,\tSpatial attention: YES","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses, metrics\n\n# Self-Attention Layer (ENABLED in Case 6)\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        shape = tf.shape(inputs)\n        height, width = shape[1], shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, shape[3]))\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        attention_output, _ = self.attention(query, key, value)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        output = self.combine_heads(concat_attention)\n        return tf.reshape(output, (batch_size, height, width, self.embed_dim))\n\n# Spatial Attention Block (ENABLED)\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch, h, w, _ = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        query = tf.reshape(query, (batch, h * w, self.filters))\n        key = tf.reshape(key, (batch, h * w, self.filters))\n        value = tf.reshape(value, (batch, h * w, self.filters))\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores /= tf.math.sqrt(tf.cast(self.filters, tf.float32))\n        attention_weights = self.softmax(attention_scores)\n        attended_features = tf.matmul(attention_weights, value)\n        attended_features = tf.reshape(attended_features, (batch, h, w, self.filters))\n        output = self.conv_out(attended_features)\n        if inputs.shape[-1] == self.filters:\n            output += inputs\n        return output\n\n# Channel Attention Block\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.avg_pool = layers.GlobalAveragePooling2D()\n        self.max_pool = layers.GlobalMaxPooling2D()\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n\n    def call(self, inputs):\n        avg = self.dense2(self.dense1(tf.expand_dims(tf.expand_dims(self.avg_pool(inputs), 1), 1)))\n        max = self.dense2(self.dense1(tf.expand_dims(tf.expand_dims(self.max_pool(inputs), 1), 1)))\n        return inputs * (avg + max)\n\n# Residual Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_channel_attention=True, use_spatial_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_channel_attention = use_channel_attention\n        self.use_spatial_attention = use_spatial_attention\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        if use_spatial_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n        if use_channel_attention:\n            self.channel_attention = ChannelAttentionBlock(filters)\n        self.relu = layers.ReLU()\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        if self.use_spatial_attention:\n            x = self.spatial_attention(x)\n        if self.use_channel_attention:\n            x = self.channel_attention(x)\n        if inputs.shape[-1] == self.filters:\n            x += inputs\n        return self.relu(x)\n\n# Main Model Builder\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    x = layers.Rescaling(1./255)(inputs)\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n    # Stage 1\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Stage 2 (2 residual blocks)\n    x = ResidualAttentionBlock(128, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_channel_attention=True, use_spatial_attention=True)(x)\n\n    # Stage 3 (2 residual blocks)\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_channel_attention=True, use_spatial_attention=True)(x)\n\n    # Stage 4 (2 residual blocks)\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    # Self-Attention enabled in Case 6\n    x = SelfAttention(embed_dim=512)(x)\n\n    # One fewer residual block to match 16 total layers\n    x = ResidualAttentionBlock(512, use_channel_attention=True, use_spatial_attention=True)(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n\n    #  Dropout rate = 0.5\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n\n    model = Model(inputs, outputs, name=\"CNN_Attn_Case6\")\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-4),\n        loss=loss_fn,\n        metrics=['accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC()]\n    )\n    return model\n\n# Run model for test\nif __name__ == \"__main__\":\n    model = build_cnn_attention_model(224, 224, 2)\n    model.summary()\n    dummy_input = tf.random.normal((1, 224, 224, 3))\n    output = model(dummy_input)\n    print(f\"\\nModel Output: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=300\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 7: Data Augmentation using Enhanced CGAN,\tDropout: 0.3,\tNo. of epochs: 200, No. of layers: 16\tSelf-attention: NO,\tSpatial attention: NO","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses, metrics\n\n# -----------------------------\n# Channel Attention Block (Unchanged, but disabled in this config)\n# -----------------------------\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n\n    def call(self, inputs):\n        avg_pool = self.global_avg_pool(inputs)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)\n        avg_pool = self.dense2(self.dense1(avg_pool))\n\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense2(self.dense1(max_pool))\n\n        attention = avg_pool + max_pool\n        return inputs * attention\n\n# -----------------------------\n# Spatial Attention Block (Unchanged, but disabled in this config)\n# -----------------------------\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size, h, w, c = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n\n        query = tf.reshape(query, (batch_size, h * w, -1))\n        key = tf.reshape(key, (batch_size, h * w, -1))\n        value = tf.reshape(value, (batch_size, h * w, -1))\n\n        score = tf.matmul(query, key, transpose_b=True)\n        score = score / tf.math.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n        weights = self.softmax(score)\n\n        attended = tf.matmul(weights, value)\n        attended = tf.reshape(attended, (batch_size, h, w, -1))\n        out = self.conv_out(attended)\n\n        if inputs.shape[-1] == out.shape[-1]:\n            out += inputs\n\n        return out\n\n# -----------------------------\n# Residual Block with toggle switches for attention\n# -----------------------------\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_spatial_attention=False, use_channel_attention=False, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_spatial_attention = use_spatial_attention\n        self.use_channel_attention = use_channel_attention\n\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if use_spatial_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n        if use_channel_attention:\n            self.channel_attention = ChannelAttentionBlock(filters)\n\n        self.relu = layers.ReLU()\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.use_spatial_attention:\n            x = self.spatial_attention(x)\n        if self.use_channel_attention:\n            x = self.channel_attention(x)\n\n        if inputs.shape[-1] == self.filters:\n            x += inputs\n\n        return self.relu(x)\n\n# -----------------------------\n# Final Model (Case 7)\n# -----------------------------\ndef build_cnn_case7_model(image_width=224, image_height=224, num_classes=2):\n    inputs = layers.Input(shape=(image_width, image_height, 3))\n    x = layers.Rescaling(1.0 / 255)(inputs)\n\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n\n    # Initial conv layers\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Residual Blocks — 16 layers total, NO attention\n    x = ResidualAttentionBlock(128, use_spatial_attention=False, use_channel_attention=False)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_spatial_attention=False, use_channel_attention=False)(x)\n\n    x = ResidualAttentionBlock(256, use_spatial_attention=False, use_channel_attention=False)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_spatial_attention=False, use_channel_attention=False)(x)\n\n    x = ResidualAttentionBlock(512, use_spatial_attention=False, use_channel_attention=False)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(512, use_spatial_attention=False, use_channel_attention=False)(x)\n\n    # Self-Attention not used in Case 7\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n\n    # Dropout = 0.3 for Case 7\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n\n    if num_classes == 2:\n        outputs = layers.Dense(1, activation='sigmoid')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(num_classes, activation='softmax')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n\n    model = Model(inputs, outputs, name=\"CNN_Case7\")\n\n    model.compile(\n        optimizer=optimizers.Adam(1e-4),\n        loss=loss_fn,\n        metrics=[\n            'accuracy',\n            metrics.Precision(name=\"precision\"),\n            metrics.Recall(name=\"recall\"),\n            metrics.AUC(name=\"auc\")\n        ]\n    )\n\n    return model\n\n# -----------------------------\n# Testing the model\n# -----------------------------\nif __name__ == \"__main__\":\n    model = build_cnn_case7_model()\n    model.summary()\n    dummy_input = tf.random.normal((1, 224, 224, 3))\n    output = model(dummy_input)\n    print(\"Output shape:\", output.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 8: Data Augmentation using Enhanced CGAN,\tDropout: 0.3\tNo. of epochs: 100, No. of layers: 20\tSelf-attention: YES,\tSpatial attention: YES","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# ================================\n# Self-Attention Layer\n# ================================\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))\n\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n\n        attention_output, _ = self.attention(query, key, value)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        output = self.combine_heads(concat_attention)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        return output\n\n# ================================\n# Spatial Attention Block\n# ================================\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size, height, width, _ = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))\n        attention_weights = self.softmax(attention_scores)\n\n        attended_features = tf.matmul(attention_weights, value)\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        output = self.conv_out(attended_features)\n\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        return output\n\n# ================================\n# Channel Attention Block\n# ================================\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n\n    def call(self, inputs):\n        avg_pool = self.global_avg_pool(inputs)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)\n        avg_pool = self.dense2(self.dense1(avg_pool))\n\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense2(self.dense1(max_pool))\n\n        attention = avg_pool + max_pool\n        return inputs * attention\n\n# ================================\n# Residual Block with Optional Attention\n# ================================\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_spatial=False, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_spatial = use_spatial\n\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if use_spatial:\n            self.spatial_attention = SpatialAttentionBlock(filters)  # Enabled Spatial Attention\n        self.channel_attention = ChannelAttentionBlock(filters)\n        self.relu = layers.ReLU()\n\n    def call(self, inputs):\n        x = self.bn1(self.conv1(inputs))\n        x = self.bn2(self.conv2(x))\n\n        if self.use_spatial:\n            x = self.spatial_attention(x)  # Apply Spatial Attention if enabled\n        x = self.channel_attention(x)      # Always apply Channel Attention\n\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        return self.relu(x)\n\n# ================================\n# Main CNN Model Definition\n# ================================\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    x = layers.Rescaling(1./255)(inputs)\n\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n    # 20 total conv/attention/residual layers including these blocks\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = ResidualAttentionBlock(128, use_spatial=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_spatial=True)(x)\n    x = ResidualAttentionBlock(128, use_spatial=True)(x)\n\n    x = ResidualAttentionBlock(256, use_spatial=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_spatial=True)(x)\n    x = ResidualAttentionBlock(256, use_spatial=True)(x)\n\n    x = ResidualAttentionBlock(512, use_spatial=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)  # Self-attention enabled\n    x = ResidualAttentionBlock(512, use_spatial=True)(x)\n    x = ResidualAttentionBlock(512, use_spatial=True)(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Dropout(0.3)(x)  # Dropout set to 0.3\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-4),\n        loss=loss_fn,\n        metrics=[\n            'accuracy',\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n# Usage example\nif __name__ == \"__main__\":\n    IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES = 224, 224, 2\n    print(\"Building CNN with Self-Attention and Spatial Attention (Case 8)...\")\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    model.summary()\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    output = model(dummy_input)\n    print(f\"\\nModel output shape: {output.shape}\")\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 9: Data Augmentation using Enhanced CGAN,\tDropout: 0.7\tNo. of epochs: 100, No. of layers: 20\tSelf-attention: YES,\tSpatial attention: YES","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# ================================\n# Self-Attention Layer\n# ================================\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))\n\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n\n        attention_output, _ = self.attention(query, key, value)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        output = self.combine_heads(concat_attention)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        return output\n\n# ================================\n# Spatial Attention Block\n# ================================\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, inputs):\n        batch_size, height, width, _ = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))\n        attention_weights = self.softmax(attention_scores)\n\n        attended_features = tf.matmul(attention_weights, value)\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        output = self.conv_out(attended_features)\n\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        return output\n\n# ================================\n# Channel Attention Block\n# ================================\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n\n    def call(self, inputs):\n        avg_pool = self.global_avg_pool(inputs)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)\n        avg_pool = self.dense2(self.dense1(avg_pool))\n\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense2(self.dense1(max_pool))\n\n        attention = avg_pool + max_pool\n        return inputs * attention\n\n# ================================\n# Residual Block\n# ================================\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_spatial=False, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_spatial = use_spatial\n\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if use_spatial:\n            self.spatial_attention = SpatialAttentionBlock(filters)  # Enabled Spatial Attention\n        self.channel_attention = ChannelAttentionBlock(filters)\n        self.relu = layers.ReLU()\n\n    def call(self, inputs):\n        x = self.bn1(self.conv1(inputs))\n        x = self.bn2(self.conv2(x))\n\n        if self.use_spatial:\n            x = self.spatial_attention(x)  # Apply Spatial Attention if enabled\n        x = self.channel_attention(x)      # Always apply Channel Attention\n\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        return self.relu(x)\n\n# ================================\n# Main CNN Model Definition\n# ================================\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    x = layers.Rescaling(1./255)(inputs)\n\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n\n    # 20 total conv/attention/residual layers including these blocks\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n\n    x = ResidualAttentionBlock(128, use_spatial=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_spatial=True)(x)\n    x = ResidualAttentionBlock(128, use_spatial=True)(x)\n\n    x = ResidualAttentionBlock(256, use_spatial=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_spatial=True)(x)\n    x = ResidualAttentionBlock(256, use_spatial=True)(x)\n\n    x = ResidualAttentionBlock(512, use_spatial=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)  # Self-attention enabled\n    x = ResidualAttentionBlock(512, use_spatial=True)(x)\n    x = ResidualAttentionBlock(512, use_spatial=True)(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Dropout(0.7)(x)  # Dropout set to 0.3\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.7)(x)\n\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-4),\n        loss=loss_fn,\n        metrics=[\n            'accuracy',\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n# Usage example\nif __name__ == \"__main__\":\n    IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES = 224, 224, 2\n    print(\"Building CNN with Self-Attention and Spatial Attention (Case 9)...\")\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    model.summary()\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    output = model(dummy_input)\n    print(f\"\\nModel output shape: {output.shape}\")\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Case 10: Data Augmentation using Enhanced CGAN,\tDropout: 0.3\tNo. of epochs: 300, No. of layers: 17\tSelf-attention: YES,\tSpatial attention: YES","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n\n\n# Load the Training and Validation Dataset\ntrain_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'training',\n    seed = 0\n)\n\nvalidation_ds = image_dataset_from_directory(\n    TRAIN_PATH,\n    labels = 'inferred',\n    validation_split = 0.3,\n    batch_size = BATCH_SIZE,\n    image_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n    subset = 'validation',\n    seed = 0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim                      # Total embedding dimension\n        self.num_heads = num_heads                      # Number of attention heads\n        self.head_dim = embed_dim // num_heads          # Dimension per attention head\n        \n        # Ensure embedding dimension is divisible by number of heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        # Dense layers to project inputs into queries, keys, and values\n        self.query_dense = layers.Dense(embed_dim)  \n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        \n        # Dense layer to combine multiple attention heads output\n        self.combine_heads = layers.Dense(embed_dim)\n        \n    def attention(self, query, key, value):\n        # Calculate scaled dot-product attention scores\n        score = tf.matmul(query, key, transpose_b=True)   # Compute dot product of query and key^T\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Dimensionality for scaling\n        scaled_score = score / tf.math.sqrt(dim_key)      # Scale scores to stabilize gradients\n        weights = tf.nn.softmax(scaled_score, axis=-1)    # Apply softmax to get attention weights\n        output = tf.matmul(weights, value)                 # Weighted sum of values based on attention weights\n        return output, weights\n    \n    def separate_heads(self, x, batch_size):\n        # Reshape the input tensor to split into multiple attention heads\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim)) \n        # Transpose to shape: (batch_size, num_heads, sequence_length, head_dim)\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        \n        # Flatten spatial dimensions (height and width) into one sequence dimension\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))  # shape: (batch, seq_len, channels)\n        \n        # Create queries, keys, and values projections\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        \n        # Separate each into multiple heads\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        \n        # Compute attention output and weights\n        attention_output, attention_weights = self.attention(query, key, value)\n        \n        # Transpose back to (batch_size, seq_len, embed_dim)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        \n        # Concatenate attention heads\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        \n        # Final dense layer to combine heads into output\n        output = self.combine_heads(concat_attention)\n        \n        # Reshape output back to spatial format (batch_size, height, width, embed_dim)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        \n        return output\n\n# Spatial Attention Block\n#SpatialAttentionBlock is designed to perform spatial attention—that is, it learns where in the image the model should \"pay more attention\" by assigning weights to each spatial location\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        # 1x1 convolutions for query, key, and value feature generation\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)  # Final convolution after attention\n        self.softmax = layers.Softmax(axis=-1)    # Softmax for attention weights\n        \n    def call(self, inputs):\n        # Get dynamic shapes for batch size, height, width, channels\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        \n        # Generate query, key, and value feature maps from input\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        \n        # Reshape to (batch_size, sequence_length, filters) for matrix multiplication\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n        \n        # Compute attention scores by dot product of query and key transpose\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))  # Scale scores\n        \n        # Softmax to obtain attention weights\n        attention_weights = self.softmax(attention_scores)\n        \n        # Weighted sum of values based on attention weights\n        attended_features = tf.matmul(attention_weights, value)\n        \n        # Reshape attended features back to spatial format\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        \n        # Final convolution to refine features\n        output = self.conv_out(attended_features)\n        \n        # Add residual connection if input channels match filter count\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        \n        return output\n\n# Channel Attention Block\n#In Channel Attention, the model learns to weigh the importance of each channel in a feature map.\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        \n        # Global average and max pooling layers\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        \n        # Dense layers to learn channel-wise attention weights\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Average pooling branch: captures average channel information\n        avg_pool = self.global_avg_pool(inputs)                     # Shape: (batch_size, channels)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)   # Reshape to (batch_size, 1, 1, channels)\n        avg_pool = self.dense1(avg_pool)                             # Reduce dimension and non-linearity\n        avg_pool = self.dense2(avg_pool)                             # Channel attention weights via sigmoid\n        \n        # Max pooling branch: captures prominent channel features\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n        \n        # Combine attention weights from both branches\n        attention = avg_pool + max_pool\n        \n        # Multiply input features by attention weights (channel-wise scaling)\n        return inputs * attention\n\n# ResNet-like Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_attention = use_attention\n        \n        # Two convolutional layers with batch normalization\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Attention blocks (spatial and channel) if enabled\n        if use_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n            self.channel_attention = ChannelAttentionBlock(filters)\n        \n        self.relu = layers.ReLU()\n        \n    def call(self, inputs):\n        # First conv layer with activation and batch norm\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        \n        # Second conv layer with batch norm (no activation yet)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        # Apply attention blocks if enabled\n        if self.use_attention:\n            x = self.spatial_attention(x)\n            x = self.channel_attention(x)\n        \n        # Residual skip connection if input and output have same channel dimension\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        \n        # Final activation after residual addition\n        return self.relu(x)\n\n# CNN Model with Self-Attention\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    x = layers.Rescaling(1./255)(inputs)\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)  # 1 conv\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    # Stage 1: 2 conv layers\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)  # 1 conv\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)  # 1 conv\n    x = layers.BatchNormalization()(x)\n    \n    # Stage 2: 2 ResidualAttentionBlocks (4 conv layers)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)  # 2 conv layers\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)  # 2 conv layers\n    \n    # Stage 3: 2 ResidualAttentionBlocks (4 conv layers)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)  # 2 conv layers\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)  # 2 conv layers\n    \n    # Stage 4: 3 ResidualAttentionBlocks (6 conv layers)\n    x = ResidualAttentionBlock(512, use_attention=True)(x)  # 2 conv layers\n    x = layers.MaxPooling2D(2)(x)\n    \n    x = SelfAttention(embed_dim=512, num_heads=8)(x)  # no conv layers\n    \n    x = ResidualAttentionBlock(512, use_attention=True)(x)  # 2 conv layers\n    x = ResidualAttentionBlock(512, use_attention=True)(x)  # 2 conv layers\n    x = ResidualAttentionBlock(512, use_attention=True)(x)  # 2 conv layers\n    \n    # Global avg pooling and classifier head\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n    \n    model = Model(inputs, outputs, name=\"CNN_SelfAttention_case10\")\n    optimizer = optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=[\n            'accuracy',\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n\n# Usage example\nif __name__ == \"__main__\":\n    # Define input image dimensions and number of classes\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n    \n    print(\"Building CNN with Self-Attention model for Case10...\")\n    \n    # Build the model\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    \n    # Print the model summary showing layers and parameters\n    model.summary()\n    \n    # Generate a dummy input tensor (batch size 1, 224x224 RGB image)\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    \n    # Run the model on dummy data to test forward pass\n    output = model(dummy_input)\n    \n    # Print output tensor shape\n    print(f\"\\nModel output shape: {output.shape}\")\n    \n    # Print actual output values (probabilities or logits)\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=300\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}