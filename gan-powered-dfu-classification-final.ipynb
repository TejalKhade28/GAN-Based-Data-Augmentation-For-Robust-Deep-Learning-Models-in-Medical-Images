{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12121159,"sourceType":"datasetVersion","datasetId":7632286},{"sourceId":12126859,"sourceType":"datasetVersion","datasetId":7636099},{"sourceId":12310440,"sourceType":"datasetVersion","datasetId":7759473},{"sourceId":12314751,"sourceType":"datasetVersion","datasetId":7762253},{"sourceId":12314835,"sourceType":"datasetVersion","datasetId":7762310},{"sourceId":12315799,"sourceType":"datasetVersion","datasetId":7762849},{"sourceId":12315834,"sourceType":"datasetVersion","datasetId":7762858},{"sourceId":12388402,"sourceType":"datasetVersion","datasetId":7811724},{"sourceId":12391826,"sourceType":"datasetVersion","datasetId":7814073}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{"id":"m3SxPS0x38S0"}},{"cell_type":"code","source":"import os\nimport time\nimport cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential, Model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport glob\n\nimport cv2\nfrom PIL import Image\nfrom keras.layers import Input, Dense, BatchNormalization, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Flatten, MaxPooling2D, Dropout, Reshape","metadata":{"id":"Gpjko3VYKFUU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing (Resizing and Normalization)","metadata":{}},{"cell_type":"markdown","source":"# Resizing images 224x224(You can run once to create a dataset of 224x224 resized images)","metadata":{}},{"cell_type":"code","source":"from PIL import Image  # For image loading and processing\nimport os              # For directory and file path operations\n\n# Input and Output directories\ninput_dir = '/kaggle/input/dfuc2021/dataset/PartA_DFU_dataset/PartA_DFU_Dataset'  # Original dataset path\noutput_dir = '/kaggle/working/resized_dataset'  # Destination to save resized images\n\n# Class subfolders (assuming binary classification: Normal vs Abnormal)\nfolders = ['Normal', 'Abnormal']\n\n# Target size for resizing all images\ntarget_size = (224, 224)\n\n# Iterate over both folders (classes)\nfor folder in folders:\n    input_folder = os.path.join(input_dir, folder)       # e.g., /kaggle/input/.../Normal\n    output_folder = os.path.join(output_dir, folder)     # e.g., /kaggle/working/.../Normal\n    os.makedirs(output_folder, exist_ok=True)            # Create output folder if it doesn't exist\n\n    # Loop over each file in the class folder\n    for filename in os.listdir(input_folder):\n        # Process only common image formats\n        if filename.lower().endswith(('png', 'jpg', 'jpeg')):\n            input_path = os.path.join(input_folder, filename)\n            output_path = os.path.join(output_folder, filename)\n\n            try:\n                # Open image and ensure it's in RGB format\n                img = Image.open(input_path).convert('RGB')\n                \n                # Resize image to target dimensions using high-quality resampling\n                img = img.resize(target_size, Image.Resampling.LANCZOS)  # LANCZOS is preferred for quality\n\n                # Save the resized image to the output path\n                img.save(output_path)\n\n            except Exception as e:\n                # Catch and report any errors during processing\n                print(f\"Error processing {input_path}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# No. of images in each class","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport os\n\n# Set the directory to check\nbase_dir = '/kaggle/input/dfuc-parta-resized'  # or wherever your images are saved\nfolders = ['Normal', 'Abnormal']\n\n# Iterate through both folders\nfor folder in folders:\n    folder_path = os.path.join(base_dir, folder)\n    print(f\"\\n--- Image sizes in folder: {folder} ---\")\n    for filename in os.listdir(folder_path):\n        if filename.lower().endswith(('png', 'jpg', 'jpeg')):\n            path = os.path.join(folder_path, filename)\n            try:\n                img = Image.open(path)\n                print(f\"{filename}: {img.size}\")  # prints size like (224, 224)\n            except Exception as e:\n                print(f\"Error with {filename}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download the resized_dataset","metadata":{}},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/resized_dataset\", 'zip', \"/kaggle/working/resized_dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sample Images Abnormal","metadata":{"id":"On93eBgp38S6"}},{"cell_type":"code","source":"# Define the path to the dataset\nPATH1 = '/kaggle/input/dfuc-parta-resized/Abnormal'\n\n# List all images in the folder\nimages = os.listdir(PATH1)\nprint(f'There are {len(images)} pictures of Abnormal.')\n\n# Create a grid of 3x3 images\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n\nfor indx, axis in enumerate(axes.flatten()):\n    # Randomly select an image\n    rnd_indx = np.random.randint(0, len(images))\n    img_path = os.path.join(PATH1, images[rnd_indx])  # Construct the correct file path\n    img = plt.imread(img_path)  # Read the image\n    imgplot = axis.imshow(img)  # Display the image\n    axis.set_title(images[rnd_indx])  # Set the title to the image filename\n    axis.set_axis_off()  # Turn off axis ticks and labels\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"id":"nq0RUPwI38S9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sample images Normal","metadata":{"id":"C4XurvGX38S_"}},{"cell_type":"code","source":"# Define the path to the dataset\nPATH1 = '/kaggle/input/dfuc-parta-resized/Normal'\n\n# List all images in the folder\nimages = os.listdir(PATH1)\nprint(f'There are {len(images)} pictures of Normal.')\n\n# Create a grid of 3x3 images\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n\nfor indx, axis in enumerate(axes.flatten()):\n    # Randomly select an image\n    rnd_indx = np.random.randint(0, len(images))\n    img_path = os.path.join(PATH1, images[rnd_indx])  # Construct the correct file path\n    img = plt.imread(img_path)  # Read the image\n    imgplot = axis.imshow(img)  # Display the image\n    axis.set_title(images[rnd_indx])  # Set the title to the image filename\n    axis.set_axis_off()  # Turn off axis ticks and labels\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"id":"xP530UaD38TB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Runtime Configurations","metadata":{"id":"UFXurG7ZrcT5"}},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/dfuc-parta-resized'\n\nBATCH_SIZE = 32\nIMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\nNUM_CLASSES = 2\nEPOCHS = 200","metadata":{"id":"ydHTHf0arIfE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization of images from different folders","metadata":{"id":"MgvjvSOcr70G"}},{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\n\n# Path to your dataset (containing two subfolders: Normal/ and Abnormal/)\ndata_dir = \"/kaggle/input/dfuc-parta-resized\"  # Change to your dataset path\n\n# Load dataset without any transform\ntrain_data = ImageFolder(root=data_dir)\n\n# Get class names (folder names)\nclass_names = train_data.classes\n\n# Randomly select 10 image indices\nindices = random.sample(range(len(train_data)), 10)\n\n# Plot\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor i, idx in enumerate(indices):\n    path, label = train_data.samples[idx]  # This gives image path and label\n    img = Image.open(path).convert(\"RGB\")  # Open and ensure 3 channels\n\n    axes[i].imshow(img)\n    axes[i].set_title(f\"Label: {class_names[label]}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print class name details\nprint(\"Class Names:\", class_names)\nprint(\"Index of 'Abnormal':\", class_names.index(\"Abnormal\"))\nprint(\"Index of 'Normal':\", class_names.index(\"Normal\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing Data Imbalance","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf                  \nimport matplotlib.pyplot as plt         \nimport numpy as np                    \n\n# Define the directory path to the dataset\nTRAIN_PATH = '/kaggle/input/dfuc-parta-resized'\n\n# Automatically infers labels from subfolder names\n# Loads images in batches of 32, resizes them to specified dimensions\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',                   # Labels are inferred from folder names\n    batch_size=32,                       # Number of images per batch\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),  # Resize all images to this size\n    seed=0                               # Seed for consistent shuffling\n)\n\n# Initialize an array to store class counts: index 0 for 'Abnormal', index 1 for 'Normal'\ntotal_counts = np.array([0, 0])  # Format: [count of class 0, count of class 1]\n\n# Loop through all batches in the dataset\nfor _, labels in full_ds:\n    # Get unique class labels and their counts in this batch\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    \n    # Add batch counts to the running total for each class\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names (must match folder names or their inferred order)\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Define bar colors for each class: red and blue\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\n\n# Create a bar plot to show number of samples per class\nplt.figure(figsize=(8, 6))                   # Set figure size\nplt.bar(class_names, total_counts, color=colors)  # Plot the class counts\nplt.xlabel(\"Class\")                         # X-axis label\nplt.ylabel(\"Number of Samples\")             # Y-axis label\nplt.title(\"Class Distribution in Original Dataset\")  # Plot title\nplt.tight_layout()                          # Adjust layout to prevent clipping\nplt.show()                                  # Display the plot\n\n# Print the total count of images in each class\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")","metadata":{"id":"dBuuu00CsWPx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Weights Initialization","metadata":{"id":"9PRENT-a38TI"}},{"cell_type":"code","source":"def weights_init(m):\n    \"\"\"\n    Takes as input a neural network m that will initialize all its weights.\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)  #Stabilizes early training, avoids large weights\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)   #Keeps initial scaling/shift minimal\n        m.bias.data.fill_(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation using Enhanced Conditional GAN (Training)","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn  # Neural network components\nimport torch.optim as optim  \nfrom torch.utils.data import DataLoader  \nfrom torchvision import datasets, transforms  \nimport os\nimport matplotlib.pyplot as plt\n\n# Choose GPU if available, else CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Generator Definition\n# --------------------------------\nclass Generator(nn.Module):\n    def __init__(self, nz=128, channels=3, num_classes=2):\n        super(Generator, self).__init__()\n        self.nz = nz\n        self.label_embedding = nn.Embedding(num_classes, nz)\n\n        def block(in_feat, out_feat, k_size=4, stride=2, padding=1, dropout=False):\n            layers = [\n                nn.ConvTranspose2d(in_feat, out_feat, k_size, stride, padding, bias=False),\n                nn.BatchNorm2d(out_feat),\n                nn.ReLU(True)\n            ]\n            if dropout:\n                layers.append(nn.Dropout(0.3))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(nz * 2, 1024, 4, 1, 0),   # Input channels = nz*2=256 (noise + label), output 1024 channels; kernel=4, stride=1, padding=0; output size: 1x1 → 4x4 feature map # 1x1 → 4x4\n            *block(1024, 512),              # 4x4 → 8x8\n            *block(512, 256),               # 8x8 → 16x16\n            *block(256, 128),               # 16x16 → 32x32\n            *block(128, 64),                # 32x32 → 64x64\n            *block(64, 32),                 # 64x64 → 128x128\n            *block(32, 16),                 # 128x128 → 256x256\n            nn.ConvTranspose2d(16, channels, 4, 2, 1),  # 256x256 → 512x512\n            nn.Upsample(size=(224, 224)),              # Resize final output to 224×224\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        embed = self.label_embedding(labels)\n        x = torch.cat((z, embed), dim=1).view(-1, self.nz * 2, 1, 1)\n        return self.model(x)\n# --------------------------------\n# Discriminator Definition\n# --------------------------------\nclass Discriminator(nn.Module):\n    def __init__(self, channels=3, num_classes=2, img_size=224):\n        super(Discriminator, self).__init__()\n        self.label_embedding = nn.Embedding(num_classes, img_size * img_size)  # Embed label as image shape\n\n        def block(in_feat, out_feat, k_size=4, stride=2, padding=1, bn=False, dropout=False):\n            layers = [nn.Conv2d(in_feat, out_feat, k_size, stride, padding, bias=False)]\n            if bn:\n                layers.append(nn.BatchNorm2d(out_feat))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            if dropout:\n                layers.append(nn.Dropout(0.3))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(channels + 1, 64, dropout=True),       # Conv2d: input channels + 1 (for label), output 64; dropout applied; spatial downsample by 2 (default stride=2)\n            *block(64, 128, bn=True, dropout=True),       # Conv2d: 64 → 128 channels; batch norm + dropout; downsample spatially by 2\n            *block(128, 256, bn=True, dropout=True),      # Conv2d: 128 → 256 channels; batch norm + dropout; downsample spatially by 2\n            *block(256, 512, bn=True, dropout=True),      # Conv2d: 256 → 512 channels; batch norm + dropout; downsample spatially by 2\n            nn.Conv2d(512, 1, 4, 1, 0),                   # Conv2d: reduce channels to 1 with kernel 4, stride 1, no padding; reduces spatial size further\n            nn.AdaptiveAvgPool2d(1),                       # Adaptive average pool to make output spatial size 1x1 (scalar per batch)\n            nn.Sigmoid()    \n        )\n\n    def forward(self, img, labels):\n        embed = self.label_embedding(labels).view(-1, 1, 224, 224)  # Match image size\n        x = torch.cat((img, embed), dim=1)  # Concatenate label as extra channel\n        return self.model(x).view(-1, 1)    # Output shape: [B, 1]\n\n\n\n# --------------------------------\n# Weight Initialization Function\n# --------------------------------\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)  # Initialize Conv/Linear weights\n    elif classname.find(\"BatchNorm\") != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)  # Initialize BatchNorm weights\n        nn.init.constant_(m.bias.data, 0)  # Initialize bias\n\n# --------------------------------\n# Hyperparameters and Model Setup\n# --------------------------------\nEPOCHS = 200\nLR = 0.0002\nBATCH_SIZE = 32\nNZ = 128  # Noise dimension\nPATIENCE = 10\nIMAGE_SIZE = 224\nCHANNELS = 3\nNUM_CLASSES = 2\n\n# Instantiate models\nnetG = Generator(nz=NZ, channels=CHANNELS, num_classes=NUM_CLASSES).to(device)\nnetD = Discriminator(channels=CHANNELS, num_classes=NUM_CLASSES).to(device)\n\n# Apply custom weight initialization\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\n# Optimizers\noptimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(0.5, 0.999))\n\n# Loss function: Binary cross-entropy for real vs fake classification\ncriterion = nn.BCELoss()\n\n# --------------------------------\n# Dataset and DataLoader\n# --------------------------------\ntransform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,) * 3, (0.5,) * 3)  # Normalize to [-1, 1] for Tanh compatibility\n])\n\n# Load training data\ntrain_data = datasets.ImageFolder('/kaggle/input/dfuc-parta-resized', transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# Fixed noise and labels for evaluating generator progress during training\nfixed_noise = torch.randn(16, NZ, device=device)\nfixed_labels = torch.full((16,), 1, dtype=torch.long, device=device)\n\n# --------------------------------\n# Training Loop\n# --------------------------------\nG_losses, D_losses, D_accuracies = [], [], []  # Store loss and accuracy metrics\nbest_loss = float('inf')\ncounter = 0\n\nfor epoch in range(EPOCHS):\n    netG.train(); netD.train()\n    epoch_g_loss, epoch_d_loss, correct = 0, 0, 0\n    total = 0\n\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        batch_size = imgs.size(0)\n\n        # Ground truth labels\n        real = torch.ones(batch_size, 1, device=device)\n        fake = torch.zeros(batch_size, 1, device=device)\n\n        # ======== Train Discriminator ========\n        optimizerD.zero_grad()\n\n        # Discriminator loss on real images\n        out_real = netD(imgs, labels)\n        loss_real = criterion(out_real, real)\n\n        # Generate fake images\n        z = torch.randn(batch_size, NZ, device=device)\n        fake_labels = torch.randint(0, NUM_CLASSES, (batch_size,), device=device)\n        fake_imgs = netG(z, fake_labels)\n\n        # Discriminator loss on fake images\n        out_fake = netD(fake_imgs.detach(), fake_labels)\n        loss_fake = criterion(out_fake, fake)\n\n        d_loss = loss_real + loss_fake\n        d_loss.backward()     #backpropagation of discriminator loss\n        optimizerD.step()\n\n        # ======== Train Generator ========\n        optimizerG.zero_grad()\n        out_fake_for_g = netD(fake_imgs, fake_labels)\n        g_loss = criterion(out_fake_for_g, real)  # Generator tries to trick discriminator\n        g_loss.backward()     #backpropagation of generator loss\n        optimizerG.step()\n\n        # Accuracy of Discriminator\n        pred_real = (out_real > 0.5).float()\n        pred_fake = (out_fake < 0.5).float()\n        acc = torch.cat((pred_real, pred_fake)).sum().item()\n        correct += acc\n        total += batch_size * 2\n\n        # Track losses\n        epoch_g_loss += g_loss.item()\n        epoch_d_loss += d_loss.item()\n\n    # Compute average losses and accuracy\n    avg_g_loss = epoch_g_loss / len(train_loader)\n    avg_d_loss = epoch_d_loss / len(train_loader)\n    accuracy = 100 * correct / total\n\n    # Store metrics\n    G_losses.append(avg_g_loss)\n    D_losses.append(avg_d_loss)\n    D_accuracies.append(accuracy)\n\n    print(f\"Epoch {epoch+1}/{EPOCHS} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f} | D Acc: {accuracy:.2f}%\")\n\n\n# --------------------------------\n# Save Generator weights\n# --------------------------------\ntorch.save(netG.state_dict(), \"generator_weights_ECGAN.pth\")\nprint(\"Generator weights saved to 'generator_weights_ECGAN.pth'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting of graphs after training","metadata":{}},{"cell_type":"code","source":"# ================================\n# Plot Losses and Accuracy for generator and discriminator after training\n# ================================\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(G_losses, label=\"Generator Loss\")\nplt.plot(D_losses, label=\"Discriminator Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(D_accuracies, label=\"Discriminator Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\nplt.grid(True)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Genarating images using trained generator weights","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport torch\nimport torch.nn as nn\nimport os\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Set device to GPU if available, else CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define hyperparameters (must match training setup)\nNZ = 128            # Noise vector size\nCHANNELS = 3        # RGB image\nNUM_CLASSES = 2     # 'Abnormal'=0, 'Normal'=1\nIMAGE_SIZE = 224    # Output image dimensions\n\n# Define the Generator architecture (must match trained model)\nclass Generator(nn.Module):\n    def __init__(self, nz=128, channels=3, num_classes=2):\n        super(Generator, self).__init__()\n        self.nz = nz\n        self.label_embedding = nn.Embedding(num_classes, nz)\n\n        def block(in_feat, out_feat, k_size=4, stride=2, padding=1, dropout=False):\n            layers = [\n                nn.ConvTranspose2d(in_feat, out_feat, k_size, stride, padding, bias=False),\n                nn.BatchNorm2d(out_feat),\n                nn.ReLU(True)\n            ]\n            if dropout:\n                layers.append(nn.Dropout(0.3))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(nz * 2, 1024, 4, 1, 0),   # 1x1 → 4x4\n            *block(1024, 512),              # 4x4 → 8x8\n            *block(512, 256),               # 8x8 → 16x16\n            *block(256, 128),               # 16x16 → 32x32\n            *block(128, 64),                # 32x32 → 64x64\n            *block(64, 32),                 # 64x64 → 128x128\n            *block(32, 16),                 # 128x128 → 256x256\n            nn.ConvTranspose2d(16, channels, 4, 2, 1),  # 256x256 → 512x512\n            nn.Upsample(size=(224, 224)),              # Resize to 224×224\n            nn.Tanh()                                   # Output in [-1, 1]\n        )\n\n    def forward(self, z, labels):\n        embed = self.label_embedding(labels)\n        x = torch.cat((z, embed), dim=1).view(-1, self.nz * 2, 1, 1)\n        return self.model(x)\n\n# === Load Pre-trained Weights from Kaggle Upload ===\nweight_path = \"/kaggle/input/weights/generator_weights_ECGAN.pth\"\n\n# Instantiate the generator and load weights\nnetG = Generator(nz=NZ, channels=CHANNELS, num_classes=NUM_CLASSES).to(device)\nnetG.load_state_dict(torch.load(weight_path, map_location=device))\nnetG.eval()  # Set to evaluation mode\n\n# === Image Generation Settings ===\nnum_images = 383  # Number of images to generate\noutput_dir = \"/kaggle/working/generated_normals_using_ECGAN\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Generate noise and Normal class labels (label = 1)\nnoise = torch.randn(num_images, NZ, device=device)\nnormal_labels = torch.full((num_images,), 1, dtype=torch.long, device=device)\n\n# Generate fake images\nwith torch.no_grad():\n    fake_images = netG(noise, normal_labels)  # Output in [-1, 1]\n    fake_images = fake_images * 0.5 + 0.5     # Scale to [0, 1]\n\n# Convert tensors to images and save\nto_pil = transforms.ToPILImage()\nfor i in range(num_images):\n    img = fake_images[i].cpu()\n    img = to_pil(img)\n    img.save(f\"{output_dir}/normal_{i+1:03d}.png\")\n\nprint(f\"✅ Saved {num_images} 'Normal' images to: {output_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download the generated_normals_ECGAN as zip","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# Zip the folder\nshutil.make_archive('generated_normals_using_ECGAN', 'zip', 'generated_normals_using_ECGAN')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation using CGAN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport os\n\n# -------------Generator -------------------\nclass Generator(nn.Module):\n    def __init__(self, nz=128, channels=3, num_classes=2):\n        super(Generator, self).__init__()\n        self.nz = nz\n        self.label_embedding = nn.Embedding(num_classes, nz)\n\n        def block(in_feat, out_feat, kernel_size=4, stride=2, padding=1):\n            return [\n                nn.ConvTranspose2d(in_feat, out_feat, kernel_size, stride, padding, bias=False),\n                nn.BatchNorm2d(out_feat),\n                nn.ReLU(True)\n            ]\n\n        self.model = nn.Sequential(\n            *block(nz * 2, 1024, 4, 1, 0),   # 1x1 → 4x4\n            *block(1024, 512),              # 4x4 → 8x8\n            *block(512, 256),              # 8x8 → 16x16\n            *block(256, 128),              # 16x16 → 32x32\n            *block(128, 64),               # 32x32 → 64x64\n            *block(64, 32),                # 64x64 → 128x128\n            *block(32, 16),                # 128x128 → 256x256\n            nn.ConvTranspose2d(16, channels, 4, 2, 1),  # 256x256 → 512x512\n            nn.Upsample(size=(224, 224)),              # Resize down to 224x224\n            nn.Tanh()\n        )\n\n    def forward(self, noise, labels):\n        label_emb = self.label_embedding(labels)\n        x = torch.cat([noise, label_emb], dim=1).view(-1, self.nz * 2, 1, 1)\n        return self.model(x)\n\n#  ------------- Discriminator -------------\nclass Discriminator(nn.Module):\n    def __init__(self, channels=3, num_classes=2, image_size=224):  # ✅ Use 224\n        super(Discriminator, self).__init__()\n        self.label_embedding = nn.Embedding(num_classes, image_size * image_size)  # ✅ 224x224 = 50176\n\n        def block(in_feat, out_feat, kernel_size=4, stride=2, padding=1, bn=True):\n            layers = [nn.Conv2d(in_feat, out_feat, kernel_size, stride, padding, bias=False)]\n            if bn:\n                layers.append(nn.BatchNorm2d(out_feat))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(channels + 1, 64, bn=False),  # 224 → 112\n            *block(64, 128),                     # 112 → 56\n            *block(128, 256),                    # 56 → 28\n            *block(256, 512),                    # 28 → 14\n            *block(512, 1024),                   # 14 → 7\n            nn.Conv2d(1024, 1, 7, 1, 0)          # 7x7 → 1x1 output\n        )\n\n    def forward(self, imgs, labels):\n        label_emb = self.label_embedding(labels).view(-1, 1, 224, 224)  # Reshape correctly\n        x = torch.cat([imgs, label_emb], dim=1)\n        out = self.model(x)\n        return out.view(-1, 1)  # final logits\n\n\n#  -------------Weight Initialization -------------\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n#  ------------- Hyperparameters  -------------\nEPOCHS = 200\nBATCH_SIZE = 32\nLR = 0.0002\nNZ = 128\nCHANNELS = 3\nNUM_CLASSES = 2\nIMAGE_SIZE = 224\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -------------DataLoader  -------------\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,) * 3, (0.5,) * 3)\n])\n\ndataset = datasets.ImageFolder('/kaggle/input/dfuc-parta-resized', transform=transform)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\n#  ------------- Models, Loss, Optimizers  -------------\nnetG = Generator(nz=NZ, channels=CHANNELS, num_classes=NUM_CLASSES).to(device)\nnetD = Discriminator(channels=CHANNELS, num_classes=NUM_CLASSES, image_size=IMAGE_SIZE).to(device)\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(0.5, 0.999))\n\n#  ------------- Training Loop  -------------\nos.makedirs(\"cgan_samples\", exist_ok=True)\nfor epoch in range(EPOCHS):\n    g_loss_total, d_loss_total, correct, total = 0, 0, 0, 0\n    for imgs, labels in dataloader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        batch_size = imgs.size(0)\n\n        real = torch.ones(batch_size, 1, device=device)\n        fake = torch.zeros(batch_size, 1, device=device)\n\n        # Train Discriminator\n        optimizerD.zero_grad()\n        real_output = netD(imgs, labels)\n        d_real_loss = criterion(real_output, real)\n\n        noise = torch.randn(batch_size, NZ, device=device)\n        fake_labels = torch.randint(0, NUM_CLASSES, (batch_size,), device=device)\n        fake_imgs = netG(noise, fake_labels)\n\n        fake_output = netD(fake_imgs.detach(), fake_labels)\n        d_fake_loss = criterion(fake_output, fake)\n\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        optimizerD.step()\n\n        # Train Generator\n        optimizerG.zero_grad()\n        fake_output = netD(fake_imgs, fake_labels)\n        g_loss = criterion(fake_output, real)\n        g_loss.backward()\n        optimizerG.step()\n\n        # Accuracy\n        pred_real = (real_output >= 0).float()\n        pred_fake = (fake_output < 0).float()\n        correct += pred_real.sum().item() + pred_fake.sum().item()\n        total += 2 * batch_size\n\n        g_loss_total += g_loss.item()\n        d_loss_total += d_loss.item()\n\n    avg_g_loss = g_loss_total / len(dataloader)\n    avg_d_loss = d_loss_total / len(dataloader)\n    d_acc = 100 * correct / total\n    print(f\"[Epoch {epoch+1}/{EPOCHS}] G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f} | D Acc: {d_acc:.2f}%\")\n\n    # Save generated samples\n    if (epoch + 1) % 10 == 0:\n        netG.eval()\n        with torch.no_grad():\n            z = torch.randn(16, NZ, device=device)\n            labels = torch.tensor([1]*16, device=device)\n            samples = netG(z, labels)\n            samples = (samples + 1) / 2\n            save_image(samples, f\"cgan_samples/epoch_{epoch+1:03d}.png\", nrow=4)\n        netG.train()\n\n# -------------Save Model  -------------\ntorch.save(netG.state_dict(), \"generator_weights_CGAN_stable.pth\")\nprint(\"✅ Stable Generator weights saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using generator_weights_CGAN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport os\n\n# -------------Generator Definition  -------------\nclass Generator(nn.Module):\n    def __init__(self, nz=128, channels=3, num_classes=2):\n        super(Generator, self).__init__()\n        self.nz = nz\n        self.label_embedding = nn.Embedding(num_classes, nz)\n\n        def block(in_feat, out_feat, kernel_size=4, stride=2, padding=1):\n            return [\n                nn.ConvTranspose2d(in_feat, out_feat, kernel_size, stride, padding, bias=False),\n                nn.BatchNorm2d(out_feat),\n                nn.ReLU(True)\n            ]\n\n        self.model = nn.Sequential(\n            *block(nz * 2, 1024, 4, 1, 0),   # 1x1 → 4x4\n            *block(1024, 512),              # 4x4 → 8x8\n            *block(512, 256),              # 8x8 → 16x16\n            *block(256, 128),              # 16x16 → 32x32\n            *block(128, 64),               # 32x32 → 64x64\n            *block(64, 32),                # 64x64 → 128x128\n            *block(32, 16),                # 128x128 → 256x256\n            nn.ConvTranspose2d(16, channels, 4, 2, 1),  # 256x256 → 512x512\n            nn.Upsample(size=(224, 224)),              # Resize down to 224x224\n            nn.Tanh()\n        )\n\n    def forward(self, noise, labels):\n        label_emb = self.label_embedding(labels)\n        x = torch.cat([noise, label_emb], dim=1).view(-1, self.nz * 2, 1, 1)\n        return self.model(x)\n\n#  ------------- Parameters  -------------\nNZ = 128\nNUM_CLASSES = 2\nIMAGE_SIZE = 224\nCHANNELS = 3\nNUM_IMAGES = 383  # Change as needed\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#  -------------Load Generator -------------\nnetG = Generator(nz=NZ, channels=CHANNELS, num_classes=NUM_CLASSES).to(device)\nnetG.load_state_dict(torch.load(\"/kaggle/input/weights-cgan/generator_weights_CGAN_stable.pth\", map_location=device))\nnetG.eval()\n\n#  ------------- Generate Noise & Labels  -------------\nnoise = torch.randn(NUM_IMAGES, NZ, device=device)\nlabels = torch.full((NUM_IMAGES,), 1, dtype=torch.long, device=device)  # class 1 = Normal\n\n# -------------Generate Images -------------\nwith torch.no_grad():\n    fake_images = netG(noise, labels)\n    fake_images = (fake_images + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n\n#  ------------- Save Images  -------------\nos.makedirs(\"generated_images_CGAN\", exist_ok=True)\nfor i in range(NUM_IMAGES):\n    save_image(fake_images[i], f\"generated_images_CGAN/image_{i+1:03d}.png\")\n\nprint(f\"Saved {NUM_IMAGES} images to 'generated_images_CGAN/' folder.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# Zip the folder\nshutil.make_archive('generated_images_CGAN', 'zip', 'generated_images_CGAN')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plottingof Loss Curves","metadata":{}},{"cell_type":"code","source":"\n\n# Plot Losses and Accuracy\n #-------------\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(G_losses, label=\"Generator Loss\")\nplt.plot(D_losses, label=\"Discriminator Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(D_accuracies, label=\"Discriminator Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\nplt.grid(True)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation of generated images","metadata":{}},{"cell_type":"code","source":"pip install torch torchvision piq pytorch-fid scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FID Calculation between CGAN AND Normal","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import Dataset, DataLoader\nfrom piq import FID\nfrom torchvision.models import inception_v3\n\n#  ------------- 1. Paths to real and generated image folders  -------------\nreal_folder = '/kaggle/input/dfuc-parta-resized/Normal'\ngen_folder = '/kaggle/input/resized-dataset-cgan/resized_dataset_CGAN/Normal'\n\n#  -------------2. Image Transform  -------------\ntransform = transforms.Compose([\n    # transforms.Resize((299, 299)),  # Required size for InceptionV3\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Inception normalization\n                         std=[0.229, 0.224, 0.225]),\n])\n\n# ========== 3. Custom Dataset ==========\nclass ImageFolderDataset(Dataset):\n    def __init__(self, folder, transform):\n        self.files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img = default_loader(self.files[idx])  # Loads image with PIL\n        return self.transform(img)\n\n# ========== 4. DataLoaders ==========\nreal_loader = DataLoader(ImageFolderDataset(real_folder, transform), batch_size=32)\ngen_loader = DataLoader(ImageFolderDataset(gen_folder, transform), batch_size=32)\n\n# ========== 5. Load InceptionV3 Model ==========\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninception = inception_v3(pretrained=True, aux_logits=True, transform_input=False)\n\ninception.fc = torch.nn.Identity()  # Remove classification head\ninception.eval()\ninception = inception.to(device)\n\n#  ------------- 6. Feature Extraction Function  -------------\ndef extract_features(loader):\n    features = []\n    with torch.no_grad():\n        for imgs in loader:\n            imgs = imgs.to(device)\n            if imgs.size(1) == 1:\n                imgs = imgs.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n            feats = inception(imgs)\n            features.append(feats.cpu())\n    return torch.cat(features, dim=0)\n\n# ------------- 7. Compute FID  -------------\nprint(\"Extracting features for real images...\")\nreal_feats = extract_features(real_loader)\nprint(\"Extracting features for generated images...\")\ngen_feats = extract_features(gen_loader)\n\nfid = FID()\nfid_score = fid(gen_feats, real_feats)\nprint(f\"\\n✅ FID Score: {fid_score.item():.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FID Calculation between Enhanced CGAN AND Normal","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import Dataset, DataLoader\nfrom piq import FID\nfrom torchvision.models import inception_v3\n\n# ------------- 1. Paths to real and generated image folders  -------------\nreal_folder = '/kaggle/input/dfuc-parta-resized/Normal'\ngen_folder = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN/Normal'\n\n# ------------- 2. Image Transform  -------------\ntransform = transforms.Compose([\n    # transforms.Resize((299, 299)),  # Required size for InceptionV3\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Inception normalization\n                         std=[0.229, 0.224, 0.225]),\n])\n\n# ------------- 3. Custom Dataset -------------\nclass ImageFolderDataset(Dataset):\n    def __init__(self, folder, transform):\n        self.files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img = default_loader(self.files[idx])  # Loads image with PIL\n        return self.transform(img)\n\n#  ------------- 4. DataLoaders -------------\nreal_loader = DataLoader(ImageFolderDataset(real_folder, transform), batch_size=32)\ngen_loader = DataLoader(ImageFolderDataset(gen_folder, transform), batch_size=32)\n\n# ------------- 5. Load InceptionV3 Model  -------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninception = inception_v3(pretrained=True, aux_logits=True, transform_input=False)\n\ninception.fc = torch.nn.Identity()  # Remove classification head\ninception.eval()\ninception = inception.to(device)\n\n#  ------------- 6. Feature Extraction Function  -------------\ndef extract_features(loader):\n    features = []\n    with torch.no_grad():\n        for imgs in loader:\n            imgs = imgs.to(device)\n            if imgs.size(1) == 1:\n                imgs = imgs.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n            feats = inception(imgs)\n            features.append(feats.cpu())\n    return torch.cat(features, dim=0)\n\n#  ------------- 7. Compute FID  -------------\nprint(\"Extracting features for real images...\")\nreal_feats = extract_features(real_loader)\nprint(\"Extracting features for generated images...\")\ngen_feats = extract_features(gen_loader)\n\nfid = FID()\nfid_score = fid(gen_feats, real_feats)\nprint(f\"\\n✅ FID Score: {fid_score.item():.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SSIM calculation between normal and generated normal images from CGAN","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom skimage.metrics import structural_similarity as compare_ssim\n\nreal_folder = '/kaggle/input/dfuc-parta-resized/Normal'\ngen_folder = '/kaggle/input/383-generated/383_generated'\n\n#  Image transformation (resize and convert to tensor)\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# 3. Dataset class to pair real and generated images\nclass PairedImageDataset(Dataset):\n    def __init__(self, real_dir, gen_dir, transform=None):\n        self.real_files = sorted([f for f in os.listdir(real_dir) if f.endswith(('jpg', 'png'))])\n        self.gen_files = sorted([f for f in os.listdir(gen_dir) if f.endswith(('jpg', 'png'))])\n        self.real_dir = real_dir\n        self.gen_dir = gen_dir\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.real_files), len(self.gen_files))\n\n    def __getitem__(self, idx):\n        real_path = os.path.join(self.real_dir, self.real_files[idx])\n        gen_path = os.path.join(self.gen_dir, self.gen_files[idx])\n\n        real_img = default_loader(real_path)\n        gen_img = default_loader(gen_path)\n\n        if self.transform:\n            real_img = self.transform(real_img)\n            gen_img = self.transform(gen_img)\n\n        return real_img, gen_img\n\n# 4. Load paired images\ndataset = PairedImageDataset(real_folder, gen_folder, transform)\nloader = DataLoader(dataset, batch_size=1)\n\n# 5. Compute SSIM for each image pair\nssim_scores = []\n\nfor real, gen in loader:\n    real = real.squeeze().numpy()\n    gen = gen.squeeze().numpy()\n\n    # Convert from (C, H, W) to (H, W, C)\n    real = np.transpose(real, (1, 2, 0)).clip(0, 1)\n    gen = np.transpose(gen, (1, 2, 0)).clip(0, 1)\n\n    # If grayscale, use single channel SSIM\n    if real.shape[2] == 1 or gen.shape[2] == 1:\n        score = compare_ssim(real[:, :, 0], gen[:, :, 0], data_range=1.0)\n    else:\n        score = compare_ssim(real, gen, channel_axis=2, data_range=1.0)\n\n    ssim_scores.append(score)\n\n# 6. Average SSIM\navg_ssim = np.mean(ssim_scores)\nprint(f\"Average SSIM: {avg_ssim:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SSIM calculation between normal and generated normal images from Enhanced CGAN","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom skimage.metrics import structural_similarity as compare_ssim\n\nreal_folder = '/kaggle/input/dfuc-parta-resized/Normal'\ngen_folder = '/kaggle/input/383-generated-ecgan'\n\n#  Image transformation (resize and convert to tensor)\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# 3. Dataset class to pair real and generated images\nclass PairedImageDataset(Dataset):\n    def __init__(self, real_dir, gen_dir, transform=None):\n        self.real_files = sorted([f for f in os.listdir(real_dir) if f.endswith(('jpg', 'png'))])\n        self.gen_files = sorted([f for f in os.listdir(gen_dir) if f.endswith(('jpg', 'png'))])\n        self.real_dir = real_dir\n        self.gen_dir = gen_dir\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.real_files), len(self.gen_files))\n\n    def __getitem__(self, idx):\n        real_path = os.path.join(self.real_dir, self.real_files[idx])\n        gen_path = os.path.join(self.gen_dir, self.gen_files[idx])\n\n        real_img = default_loader(real_path)\n        gen_img = default_loader(gen_path)\n\n        if self.transform:\n            real_img = self.transform(real_img)\n            gen_img = self.transform(gen_img)\n\n        return real_img, gen_img\n\n# 4. Load paired images\ndataset = PairedImageDataset(real_folder, gen_folder, transform)\nloader = DataLoader(dataset, batch_size=1)\n\n# 5. Compute SSIM for each image pair\nssim_scores = []\n\nfor real, gen in loader:\n    real = real.squeeze().numpy()\n    gen = gen.squeeze().numpy()\n\n    # Convert from (C, H, W) to (H, W, C)\n    real = np.transpose(real, (1, 2, 0)).clip(0, 1)\n    gen = np.transpose(gen, (1, 2, 0)).clip(0, 1)\n\n    # If grayscale, use single channel SSIM\n    if real.shape[2] == 1 or gen.shape[2] == 1:\n        score = compare_ssim(real[:, :, 0], gen[:, :, 0], data_range=1.0)\n    else:\n        score = compare_ssim(real, gen, channel_axis=2, data_range=1.0)\n\n    ssim_scores.append(score)\n\n# 6. Average SSIM\navg_ssim = np.mean(ssim_scores)\nprint(f\"Average SSIM: {avg_ssim:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PSNR calculation between normal and generated normal images from CGAN","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport cv2\nfrom math import log10\n\n# === 1. Folder paths ===\nreal_folder = '/kaggle/input/dfuc-parta-resized/Normal'\ngen_folder = '/kaggle/input/383-generated/383_generated'\n\n# === 2. Image transformation ===\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# === 3. Dataset class to pair images ===\nclass PairedImageDataset(Dataset):\n    def __init__(self, real_dir, gen_dir, transform=None):\n        self.real_files = sorted([f for f in os.listdir(real_dir) if f.endswith(('jpg', 'png'))])\n        self.gen_files = sorted([f for f in os.listdir(gen_dir) if f.endswith(('jpg', 'png'))])\n        self.real_dir = real_dir\n        self.gen_dir = gen_dir\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.real_files), len(self.gen_files))\n\n    def __getitem__(self, idx):\n        real_path = os.path.join(self.real_dir, self.real_files[idx])\n        gen_path = os.path.join(self.gen_dir, self.gen_files[idx])\n\n        real_img = default_loader(real_path)\n        gen_img = default_loader(gen_path)\n\n        if self.transform:\n            real_img = self.transform(real_img)\n            gen_img = self.transform(gen_img)\n\n        return real_img, gen_img\n\n# === 4. Load paired images ===\ndataset = PairedImageDataset(real_folder, gen_folder, transform)\nloader = DataLoader(dataset, batch_size=1)\n\n# === 5. PSNR calculation function ===\ndef calculate_psnr(img1, img2):\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')  # Identical images\n    return 20 * log10(1.0 / np.sqrt(mse))  # PSNR in dB\n\n# === 6. Compute PSNR for each pair ===\npsnr_scores = []\n\nfor real, gen in loader:\n    real = real.squeeze().numpy()\n    gen = gen.squeeze().numpy()\n\n    # Convert to shape (H, W, C) and clip to [0, 1]\n    real = np.transpose(real, (1, 2, 0)).clip(0, 1)\n    gen = np.transpose(gen, (1, 2, 0)).clip(0, 1)\n\n    # Compute PSNR\n    psnr = calculate_psnr(real, gen)\n    psnr_scores.append(psnr)\n\n# === 7. Average PSNR ===\navg_psnr = np.mean(psnr_scores)\nprint(f\"✅ Average PSNR: {avg_psnr:.2f} dB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PSNR calculation between normal and generated normal images from Enhanced CGAN","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport cv2\nfrom math import log10\n\n# === 1. Folder paths ===\nreal_folder = '/kaggle/input/dfuc-parta-resized/Normal'\ngen_folder = '/kaggle/input/383-generated-ecgan'\n\n# === 2. Image transformation ===\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# === 3. Dataset class to pair images ===\nclass PairedImageDataset(Dataset):\n    def __init__(self, real_dir, gen_dir, transform=None):\n        self.real_files = sorted([f for f in os.listdir(real_dir) if f.endswith(('jpg', 'png'))])\n        self.gen_files = sorted([f for f in os.listdir(gen_dir) if f.endswith(('jpg', 'png'))])\n        self.real_dir = real_dir\n        self.gen_dir = gen_dir\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.real_files), len(self.gen_files))\n\n    def __getitem__(self, idx):\n        real_path = os.path.join(self.real_dir, self.real_files[idx])\n        gen_path = os.path.join(self.gen_dir, self.gen_files[idx])\n\n        real_img = default_loader(real_path)\n        gen_img = default_loader(gen_path)\n\n        if self.transform:\n            real_img = self.transform(real_img)\n            gen_img = self.transform(gen_img)\n\n        return real_img, gen_img\n\n# === 4. Load paired images ===\ndataset = PairedImageDataset(real_folder, gen_folder, transform)\nloader = DataLoader(dataset, batch_size=1)\n\n# === 5. PSNR calculation function ===\ndef calculate_psnr(img1, img2):\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')  # Identical images\n    return 20 * log10(1.0 / np.sqrt(mse))  # PSNR in dB\n\n# === 6. Compute PSNR for each pair ===\npsnr_scores = []\n\nfor real, gen in loader:\n    real = real.squeeze().numpy()\n    gen = gen.squeeze().numpy()\n\n    # Convert to shape (H, W, C) and clip to [0, 1]\n    real = np.transpose(real, (1, 2, 0)).clip(0, 1)\n    gen = np.transpose(gen, (1, 2, 0)).clip(0, 1)\n\n    # Compute PSNR\n    psnr = calculate_psnr(real, gen)\n    psnr_scores.append(psnr)\n\n# === 7. Average PSNR ===\navg_psnr = np.mean(psnr_scores)\nprint(f\"✅ Average PSNR: {avg_psnr:.2f} dB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Classification**","metadata":{"id":"MOtJAtuKNihn"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import EfficientNetB0, ResNet50\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import callbacks\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os","metadata":{"id":"kFrRqw1KTyld","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Loading**","metadata":{"id":"Iwgy6-6R_EVW"}},{"cell_type":"code","source":"# Define Constants\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\nBATCH_SIZE = 32\nIMAGE_HEIGHT = 224\nIMAGE_WIDTH = 224\nNUM_CLASSES = 2\nEPOCHS = 200","metadata":{"id":"iJ61byEvHhNJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Define your dataset directory\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"  # Replace with the actual path\n\n# Define preprocessing transformations\npreprocess_transforms = transforms.Compose([\n    transforms.ToTensor(),                       # Convert image to tensor\n    transforms.Normalize((0.5, 0.5, 0.5),         # Normalize to [-1, 1]\n                         (0.5, 0.5, 0.5))\n])\n\n# Apply transforms using ImageFolder\npreprocessed_dataset = datasets.ImageFolder(root=data_dir, transform=preprocess_transforms)\n\n# Create a DataLoader to view and use the data\nloader = DataLoader(preprocessed_dataset, batch_size=32, shuffle=True)\n\n# View class names\nclass_names = preprocessed_dataset.classes\nprint(f\"Classes: {class_names}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize 8 random images from a preprocessed PyTorch dataset","metadata":{}},{"cell_type":"code","source":"# Function to display a tensor image using matplotlib\ndef imshow(img_tensor, title):\n    # Convert the image tensor to a NumPy array and rearrange dimensions from (C, H, W) to (H, W, C)\n    img = img_tensor.numpy().transpose((1, 2, 0))\n\n    # De-normalize the image values from [-1, 1] range back to [0, 1] for correct display\n    img = img * 0.5 + 0.5\n\n    # Display the image using matplotlib\n    plt.imshow(img)\n\n    # Set the title (e.g., class name) above the image\n    plt.title(title)\n\n    # Hide the axes for a cleaner image display\n    plt.axis('off')\n\n\n# Select 8 random indices from the dataset for visualization\nrandom_indices = random.sample(range(len(preprocessed_dataset)), 8)\n\n\n# Create a new figure with specified size (12x6 inches)\nplt.figure(figsize=(12, 6))\n\n# Loop through the selected indices to plot each image\nfor i, idx in enumerate(random_indices):\n    # Get the image and its corresponding label from the dataset\n    image, label = preprocessed_dataset[idx]\n\n    # Create a subplot at the appropriate position in a 2-row, 4-column grid\n    plt.subplot(2, 4, i + 1)\n\n    # Display the image and its class label using the imshow() function\n    imshow(image, class_names[label])\n\n# Adjust spacing between subplots to prevent overlapping\nplt.tight_layout()\n\n# Show the final figure with all subplots\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# No. of classes","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np\nimport os\n\n# Set paths\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"  # Adjust path if needed\n\n# Image transformations\ntransform = transforms.Compose([\n    \n    transforms.ToTensor(),\n    transforms.Normalize((0.5,)*3, (0.5,)*3)\n])\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\nclass_names = full_dataset.classes  # ['Abnormal', 'Normal']\nprint(f\"Classes: {class_names}\")\n\n# Train-test split (e.g., 70% train, 30% test)\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Cleaning**","metadata":{"id":"vhY01jbE_WiG"}},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\n# Path to your dataset\ndataset_path = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Dictionary to hold extension counts per class\nextension_counts = {}\n\n# Loop through each class folder\nfor class_name in os.listdir(dataset_path):\n    class_path = os.path.join(dataset_path, class_name)\n    if os.path.isdir(class_path):\n        extensions = []\n        for file_name in os.listdir(class_path):\n            if os.path.isfile(os.path.join(class_path, file_name)):\n                ext = os.path.splitext(file_name)[-1].lower()\n                extensions.append(ext)\n        extension_counts[class_name] = Counter(extensions)\n\n# Print the results\nfor class_name, counter in extension_counts.items():\n    print(f\"\\nClass: {class_name}\")\n    for ext, count in counter.items():\n        print(f\"  {ext}: {count} files\")\n","metadata":{"id":"pWuMelnW-WSa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BALANCING DATASET","metadata":{"id":"BbiyBTdO71J8"}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the directory path to the original dataset\nTRAIN_PATH = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'\n\n# Load the entire dataset without splitting\nfull_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_PATH,\n    labels='inferred',\n    batch_size=32,  # Adjust based on your system's capability\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    seed=0\n)\n\n# Count the occurrences of each class (0: Abnormal, 1: Normal)\ntotal_counts = np.array([0, 0])  # [Count of 0s, Count of 1s]\n\n# Iterate through the dataset to count labels\nfor _, labels in full_ds:\n    unique, counts = np.unique(labels.numpy(), return_counts=True)\n    for u, c in zip(unique, counts):\n        total_counts[int(u)] += c\n\n# Define class names\nclass_names = [\"Abnormal\", \"Normal\"]\n\n# Plot the class distribution\ncolors = ['#d62728', '#1f77b4']  # Red for Abnormal, Blue for Normal\nplt.figure(figsize=(8, 6))\nplt.bar(class_names, total_counts, color=colors)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Class Distribution after balancing Original Dataset\")\nplt.tight_layout()\nplt.show()\n\n\n# Print class counts\nprint(f\"Abnormal (0): {total_counts[0]} samples\")\nprint(f\"Normal (1): {total_counts[1]} samples\")\n","metadata":{"id":"kGiaFZ6MtMWw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualization of Training Dataset**","metadata":{"id":"Btjk_JogB5yf"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))  # Adjust the figure size as needed\ngrid_size = 4  # Number of rows and columns in the grid\n\nfor images, labels in train_ds.take(1):  # Take one batch from the dataset\n    for i in range(grid_size * grid_size):  # Display grid_size^2 images\n        ax = plt.subplot(grid_size, grid_size, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n\nplt.tight_layout()  # Ensure no overlap between images and titles\nplt.show()\n","metadata":{"id":"8F0wzydXMqjy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting of Dataset (70:30)","metadata":{}},{"cell_type":"code","source":"# Import necessary modules\nimport os                    \nimport shutil                \nfrom torchvision import datasets  \n\n# Path to the original dataset (already resized and organized in subfolders by class)\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Path to where train/test split data will be stored\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")   # Path for training data\ntest_dir = os.path.join(output_dir, \"test\")     # Path for testing data\n\n# Load the full dataset using torchvision's ImageFolder\n# This assumes the dataset has subfolders named after class labels\nfull_dataset = datasets.ImageFolder(root=data_dir)\n\n# Get list of class names from the dataset (e.g., ['normal', 'abnormal'])\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create directory structure for the train and test folders\n# Example: /kaggle/working/dataset_split/train/normal\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)  # Create subdirectories per class\n\n# Split dataset into 70% training and 30% testing\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to their respective train/test folders\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:  # Use the saved indices from the random split\n        path, label = full_dataset.samples[idx]  # Get original image path and its label\n        class_name = class_names[label]          # Map label index to class name (e.g., 0 → 'normal')\n        filename = os.path.basename(path)        # Extract the filename (e.g., 'image1.png')\n        dest = os.path.join(split_folder, class_name, filename)  # Destination path for copy\n        shutil.copy2(path, dest)  # Copy image file while preserving metadata\n\n# Save the training images to train/ and test images to test/\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\n# Print confirmation messages with final paths\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Proposed CNN self-attention ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim                      # Total embedding dimension\n        self.num_heads = num_heads                      # Number of attention heads\n        self.head_dim = embed_dim // num_heads          # Dimension per attention head\n        \n        # Ensure embedding dimension is divisible by number of heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        # Dense layers to project inputs into queries, keys, and values\n        self.query_dense = layers.Dense(embed_dim)  \n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        \n        # Dense layer to combine multiple attention heads output\n        self.combine_heads = layers.Dense(embed_dim)\n        \n    def attention(self, query, key, value):\n        # Calculate scaled dot-product attention scores\n        score = tf.matmul(query, key, transpose_b=True)   # Compute dot product of query and key^T\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Dimensionality for scaling\n        scaled_score = score / tf.math.sqrt(dim_key)      # Scale scores to stabilize gradients\n        weights = tf.nn.softmax(scaled_score, axis=-1)    # Apply softmax to get attention weights\n        output = tf.matmul(weights, value)                 # Weighted sum of values based on attention weights\n        return output, weights\n    \n    def separate_heads(self, x, batch_size):\n        # Reshape the input tensor to split into multiple attention heads\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim)) \n        # Transpose to shape: (batch_size, num_heads, sequence_length, head_dim)\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        \n        # Flatten spatial dimensions (height and width) into one sequence dimension\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))  # shape: (batch, seq_len, channels)\n        \n        # Create queries, keys, and values projections\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        \n        # Separate each into multiple heads\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        \n        # Compute attention output and weights\n        attention_output, attention_weights = self.attention(query, key, value)\n        \n        # Transpose back to (batch_size, seq_len, embed_dim)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        \n        # Concatenate attention heads\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        \n        # Final dense layer to combine heads into output\n        output = self.combine_heads(concat_attention)\n        \n        # Reshape output back to spatial format (batch_size, height, width, embed_dim)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        \n        return output\n\n# Spatial Attention Block\n#SpatialAttentionBlock is designed to perform spatial attention—that is, it learns where in the image the model should \"pay more attention\" by assigning weights to each spatial location\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        # 1x1 convolutions for query, key, and value feature generation\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)  # Final convolution after attention\n        self.softmax = layers.Softmax(axis=-1)    # Softmax for attention weights\n        \n    def call(self, inputs):\n        # Get dynamic shapes for batch size, height, width, channels\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        \n        # Generate query, key, and value feature maps from input\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        \n        # Reshape to (batch_size, sequence_length, filters) for matrix multiplication\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n        \n        # Compute attention scores by dot product of query and key transpose\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))  # Scale scores\n        \n        # Softmax to obtain attention weights\n        attention_weights = self.softmax(attention_scores)\n        \n        # Weighted sum of values based on attention weights\n        attended_features = tf.matmul(attention_weights, value)\n        \n        # Reshape attended features back to spatial format\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        \n        # Final convolution to refine features\n        output = self.conv_out(attended_features)\n        \n        # Add residual connection if input channels match filter count\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        \n        return output\n\n# Channel Attention Block\n#In Channel Attention, the model learns to weigh the importance of each channel in a feature map.\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        \n        # Global average and max pooling layers\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        \n        # Dense layers to learn channel-wise attention weights\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Average pooling branch: captures average channel information\n        avg_pool = self.global_avg_pool(inputs)                     # Shape: (batch_size, channels)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)   # Reshape to (batch_size, 1, 1, channels)\n        avg_pool = self.dense1(avg_pool)                             # Reduce dimension and non-linearity\n        avg_pool = self.dense2(avg_pool)                             # Channel attention weights via sigmoid\n        \n        # Max pooling branch: captures prominent channel features\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n        \n        # Combine attention weights from both branches\n        attention = avg_pool + max_pool\n        \n        # Multiply input features by attention weights (channel-wise scaling)\n        return inputs * attention\n\n# ResNet-like Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_attention = use_attention\n        \n        # Two convolutional layers with batch normalization\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Attention blocks (spatial and channel) if enabled\n        if use_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n            self.channel_attention = ChannelAttentionBlock(filters)\n        \n        self.relu = layers.ReLU()\n        \n    def call(self, inputs):\n        # First conv layer with activation and batch norm\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        \n        # Second conv layer with batch norm (no activation yet)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        # Apply attention blocks if enabled\n        if self.use_attention:\n            x = self.spatial_attention(x)\n            x = self.channel_attention(x)\n        \n        # Residual skip connection if input and output have same channel dimension\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        \n        # Final activation after residual addition\n        return self.relu(x)\n\n# CNN Model with Self-Attention\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    \"\"\"\n    Build CNN model with self-attention mechanisms\n    \n    Args:\n        IMAGE_WIDTH: Input image width\n        IMAGE_HEIGHT: Input image height  \n        NUM_CLASSES: Number of output classes\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    \n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))  # Input layer with image shape\n    \n    # Normalize pixel values to [0,1]\n    x = layers.Rescaling(1./255)(inputs)\n    \n    # Initial convolutional layer with large kernel and stride for downsampling\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    # Stage 1: Basic convolutions with batch norm\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Stage 2: Residual blocks with attention + downsampling\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    \n    # Stage 3: Increased filter count and residual attention blocks + downsampling\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    \n    # Stage 4: High-level features + self-attention + downsampling\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Apply self-attention over feature map\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)\n    \n    # Additional residual attention blocks after self-attention\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    \n    # Global average pooling to reduce spatial dimensions\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Dropout for regularization to prevent overfitting\n    x = layers.Dropout(0.5)(x)\n    \n    # Dense layer with ReLU activation for learning complex features\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer:\n    # For binary classification, use sigmoid activation and binary crossentropy loss\n    # For multi-class, use softmax activation and sparse categorical crossentropy\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n    \n    # Create the Keras model instance\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    \n    # Compile model with Adam optimizer, selected loss, and common classification metrics\n    optimizer = optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=[\n            'accuracy',                            # Classification accuracy\n            metrics.Precision(name='precision'), # Precision metric\n            metrics.Recall(name='recall'),       # Recall metric\n            metrics.AUC(name='auc')               # Area under ROC curve\n        ]\n    )\n\n    return model\n\n\n\nif __name__ == \"__main__\":\n    # Define input image dimensions and number of classes\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n    \n    print(\"Building CNN with Self-Attention model...\")\n    \n    # Build the model\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    \n    # Print the model summary showing layers and parameters\n    model.summary()\n    \n    # Generate a dummy input tensor (batch size 1, 224x224 RGB image)\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    \n    # Run the model on dummy data to test forward pass\n    output = model(dummy_input)\n    \n    # Print output tensor shape\n    print(f\"\\nModel output shape: {output.shape}\")\n    \n    # Print actual output values (probabilities or logits)\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf  # Import TensorFlow library\n\n# Paths to train and test directories (previously created and organized by class)\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters for image input size and batch processing\nIMAGE_SIZE = (224, 224)   # Input image size (height, width) matching model requirement\nBATCH_SIZE = 32           # Number of images per batch during training\n\n# --- Load training dataset from directory ---\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,                  # Path to training images, organized in class subfolders\n    label_mode='binary',        # Since there are only 2 classes (e.g., 'normal' and 'abnormal')\n    image_size=IMAGE_SIZE,      # All images will be resized to this shape\n    batch_size=BATCH_SIZE,      # How many images per batch\n    shuffle=True                # Shuffle the dataset for better generalization during training\n)\n\n# --- Load test dataset from directory ---\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,                   # Path to testing images\n    label_mode='binary',        # Binary classification (0 or 1)\n    image_size=IMAGE_SIZE,      # Resize test images to same size as training\n    batch_size=BATCH_SIZE,      # Process test data in batches\n    shuffle=False               # Don't shuffle test data to maintain consistent evaluation\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Used to optimize the performance of your input pipeline in TensorFlow.","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=200\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting Accuracy and Loss from training","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Import matplotlib for plotting training graphs\n\n# Create a figure with two subplots: one for accuracy, one for loss\nplt.figure(figsize=(12, 5))  # Set the overall figure size (width=12, height=5 inches)\n\nplt.subplot(1, 2, 1)  # Create first subplot (1 row, 2 columns, position 1)\n\n# Plot accuracy over epochs from the training history\nplt.plot(history.history['accuracy'], label='Accuracy')  # Plot training accuracy\nplt.title('Model Accuracy')  # Title of the subplot\nplt.xlabel('Epoch')          # X-axis label\nplt.ylabel('Accuracy')       # Y-axis label\nplt.legend()                 # Show legend (e.g., 'Val Accuracy')\nplt.grid(True)               # Add grid for better readability\n\n\nplt.subplot(1, 2, 2)  # Create second subplot (position 2)\n\n# Plot loss over epochs from the training history\nplt.plot(history.history['loss'], label='Loss')  # Plot training loss\nplt.title('Model Loss')     # Title of the subplot\nplt.xlabel('Epoch')         # X-axis label\nplt.ylabel('Loss')          # Y-axis label\nplt.legend()                # Show legend (e.g., 'Val Loss')\nplt.grid(True)              # Add grid for better readability\n\n\nplt.tight_layout()  # Automatically adjust subplot spacing for a clean layout\nplt.show()          # Display the plot window\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting val_accuracy and val_loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Import matplotlib for plotting training and validation graphs\n\n# Create a figure with two subplots: one for validation accuracy, one for validation loss\nplt.figure(figsize=(12, 5))  # Set the overall figure size (width=12, height=5 inches)\n\n# -------- Validation Accuracy Plot --------\nplt.subplot(1, 2, 1)  # Create first subplot (1 row, 2 columns, position 1)\n\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')  # Plot validation accuracy over epochs\nplt.title('Validation Accuracy')  # Title of the subplot\nplt.xlabel('Epoch')               # X-axis label\nplt.ylabel('Accuracy')            # Y-axis label\nplt.legend()                      # Show legend (e.g., 'Val Accuracy')\nplt.grid(True)                    # Add grid lines for better readability\n\n# -------- Validation Loss Plot --------\nplt.subplot(1, 2, 2)  # Create second subplot (1 row, 2 columns, position 2)\n\nplt.plot(history.history['val_loss'], label='Val Loss')  # Plot validation loss over epochs\nplt.title('Validation Loss')      # Title of the subplot\nplt.xlabel('Epoch')               # X-axis label\nplt.ylabel('Loss')                # Y-axis label\nplt.legend()                      # Show legend (e.g., 'Val Loss')\nplt.grid(True)                    # Add grid lines for better readability\n\nplt.tight_layout()  # Automatically adjust subplot spacing for a clean layout\nplt.show()          # Display the combined figure with both plots\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Get true labels from the validation dataset\ny_true = np.concatenate([y.numpy() for x, y in validation_ds], axis=0)  # Ground truth labels\n\n# Get predicted probabilities from the model\ny_pred_probs = model.predict(validation_ds)  # Output will be probabilities between 0 and 1\n\n# Convert probabilities to binary class predictions using threshold 0.5\ny_pred = (y_pred_probs > 0.5).astype(int).flatten()  # Flatten in case output is shape (N, 1)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Abnormal (1)\"])\ndisp.plot(cmap='Blues', values_format='d')  # 'd' ensures integer values are shown\n\nplt.title(\"Confusion Matrix on Validation Set\")\nplt.grid(False)  # Disable grid for cleaner view\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ROC AUC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Get ground truth labels from the validation dataset\ny_true = np.concatenate([y.numpy() for x, y in validation_ds], axis=0)\n\n# Step 2: Get predicted probabilities from the model\ny_scores = model.predict(validation_ds).flatten()  # Flatten in case shape is (N, 1)\n\n# Step 3: Compute False Positive Rate, True Positive Rate and thresholds\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\n\n# Step 4: Compute AUC\nauc_score = roc_auc_score(y_true, y_scores)\n\n# Step 5: Plot the ROC Curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.4f})\", color='blue')\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guessing\")  # Diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparison with state-of-the-art  \n1. LeNet\n2. AlexNet\n3. GoogleNet\n4. EfficientNetB0\n5. DenseNet121\n6. ResNet101\n7. InceptionV3\n8. VGG16\n9. Proposed Model","metadata":{}},{"cell_type":"markdown","source":"# EfficientnetB0","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndef build_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n    \n    # Preprocessing layer\n    x = layers.Lambda(preprocess_input)(inputs)\n\n    # Load EfficientNetB0 base\n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=x)\n    base_model.trainable = False  # Freeze base for initial training\n\n    # Classification head\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4, name=\"top_dropout\")(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)  # Binary output\n\n    # Create model\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNetB0_Binary\")\n\n    # Compile with all important binary metrics\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model1 = build_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history1 = model1.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_accuracy_loss(history1):\n    plt.figure(figsize=(12, 5))\n\n    # Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history1['accuracy'], label='Train Accuracy')\n    plt.plot(history.history1['val_accuracy'], label='Val Accuracy')\n    plt.title('Accuracy Over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n\n    # Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history1['loss'], label='Train Loss')\n    plt.plot(history.history1['val_loss'], label='Val Loss')\n    plt.title('Loss Over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\n\ndef plot_confusion_matrix(model, validation_ds):\n    y_true = np.concatenate([y.numpy() for _, y in validation_ds])\n    y_pred_probs = model.predict(validation_ds).flatten()\n    y_pred = (y_pred_probs > 0.5).astype(int)\n\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap='Blues')\n    plt.title(\"Confusion Matrix\")\n    plt.grid(False)\n    plt.show()\n\n    return cm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Get true labels from validation dataset\ny_true = np.concatenate([y.numpy() for x, y in validation_ds], axis=0)\n\n# Get predicted probabilities\ny_pred_probs = model.predict(validation_ds)\n\n# Convert probabilities to binary predictions (threshold = 0.5)\ny_pred = (y_pred_probs > 0.5).astype(int).flatten()\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Abnormal (1)\"])\ndisp.plot(cmap='Blues')\nplt.title(\"Confusion Matrix on Validation Set\")\nplt.grid(False)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DenseNet121","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.applications.densenet import preprocess_input\n\ndef build_densenet_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n    \n    # Preprocessing layer\n    x = layers.Lambda(preprocess_input)(inputs)\n\n    # Load DenseNet121 base\n    base_model = DenseNet121(include_top=False, weights='imagenet', input_tensor=x)\n    base_model.trainable = False  # Freeze base model\n\n    # Classification head\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4, name=\"top_dropout\")(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)  # Binary output\n\n    # Create model\n    model = tf.keras.Model(inputs, outputs, name=\"DenseNet121_Binary\")\n\n    # Compile with all important binary metrics\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2 = build_densenet_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history2 = model2.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ResNet101","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics\nfrom tensorflow.keras.applications import ResNet101\nfrom tensorflow.keras.applications.resnet import preprocess_input\n\ndef build_resnet101_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n    \n    # Preprocessing layer\n    x = layers.Lambda(preprocess_input)(inputs)\n\n    # Load ResNet101 base model\n    base_model = ResNet101(include_top=False, weights='imagenet', input_tensor=x)\n    base_model.trainable = False  # Freeze base initially\n\n    # Classification head\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4, name=\"top_dropout\")(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)  # Binary output\n\n    # Create final model\n    model = tf.keras.Model(inputs, outputs, name=\"ResNet101_Binary\")\n\n    # Compile with all important binary classification metrics\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model3 = build_resnet101_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history3 = model3.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# InceptionV3","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\n\ndef build_inceptionv3_model(INPUT_HEIGHT=224, INPUT_WIDTH=224, NUM_CLASSES=1):\n    inputs = layers.Input(shape=(INPUT_HEIGHT, INPUT_WIDTH, 3))\n    \n    # Resize input images to 299x299 inside the model\n    x = layers.Resizing(299, 299)(inputs)\n    \n    # Preprocessing for InceptionV3\n    x = layers.Lambda(preprocess_input)(x)\n\n    # Load InceptionV3 base model\n    base_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=x)\n    base_model.trainable = False  # Freeze base model\n\n    # Classification head\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4, name=\"top_dropout\")(x)\n    outputs = layers.Dense(NUM_CLASSES, activation=\"sigmoid\", name=\"pred\")(x)  # Binary output\n\n    model = tf.keras.Model(inputs, outputs, name=\"InceptionV3_Binary\")\n\n    # Compile model with metrics for binary classification\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n\n\n# Example usage:\nmodel = build_inceptionv3_model(INPUT_HEIGHT=224, INPUT_WIDTH=224)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model4 = build_inceptionv3_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history4 = model4.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG16","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\ndef build_vgg16_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n\n    # Preprocessing\n    x = layers.Lambda(preprocess_input)(inputs)\n\n    # Load VGG16 base model\n    base_model = VGG16(include_top=False, weights='imagenet', input_tensor=x)\n    base_model.trainable = False  # Freeze convolutional base\n\n    # Classification head\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4, name=\"top_dropout\")(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)  # Binary output\n\n    # Build model\n    model = tf.keras.Model(inputs, outputs, name=\"VGG16_Binary\")\n\n    # Compile with binary classification metrics\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model5 = build_vgg16_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history5 = model5.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LeNet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, losses, metrics\n\ndef build_lenet_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    model = models.Sequential([\n        layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),  # RGB images\n\n        # First conv block\n        layers.Conv2D(6, kernel_size=5, activation='relu', padding='same'),\n        layers.AveragePooling2D(),\n\n        # Second conv block\n        layers.Conv2D(16, kernel_size=5, activation='relu'),\n        layers.AveragePooling2D(),\n\n        # Flatten + Dense layers\n        layers.Flatten(),\n        layers.Dense(120, activation='relu'),\n        layers.Dense(84, activation='relu'),\n        layers.Dense(1, activation='sigmoid')  # Binary classification\n    ])\n\n    # Compile model with all binary metrics\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model6 = build_vgg16_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history6 = model6.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Alexnet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, losses, metrics\n\ndef build_alexnet_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    model = models.Sequential([\n        layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n\n        # 1st Conv Layer\n        layers.Conv2D(filters=96, kernel_size=11, strides=4, activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(pool_size=3, strides=2),\n\n        # 2nd Conv Layer\n        layers.Conv2D(filters=256, kernel_size=5, padding='same', activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(pool_size=3, strides=2),\n\n        # 3rd, 4th, 5th Conv Layers\n        layers.Conv2D(filters=384, kernel_size=3, padding='same', activation='relu'),\n        layers.Conv2D(filters=384, kernel_size=3, padding='same', activation='relu'),\n        layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'),\n        layers.MaxPooling2D(pool_size=3, strides=2),\n\n        # Flatten and Fully Connected layers\n        layers.Flatten(),\n        layers.Dense(4096, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(4096, activation='relu'),\n        layers.Dropout(0.5),\n\n        # Output Layer for Binary Classification\n        layers.Dense(1, activation='sigmoid')  # Binary classification\n    ])\n\n    # Compile with binary classification metrics\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-4),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model7 = build_alexnet_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history7 = model7.fit(train_ds, validation_data = validation_ds,\n                    epochs=200, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GoogleNet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, losses, metrics, optimizers\n\ndef inception_module(x, f1, f3_in, f3_out, f5_in, f5_out, pool_proj):\n    path1 = layers.Conv2D(f1, (1,1), padding='same', activation='relu')(x)\n\n    path2 = layers.Conv2D(f3_in, (1,1), padding='same', activation='relu')(x)\n    path2 = layers.Conv2D(f3_out, (3,3), padding='same', activation='relu')(path2)\n\n    path3 = layers.Conv2D(f5_in, (1,1), padding='same', activation='relu')(x)\n    path3 = layers.Conv2D(f5_out, (5,5), padding='same', activation='relu')(path3)\n\n    path4 = layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(x)\n    path4 = layers.Conv2D(pool_proj, (1,1), padding='same', activation='relu')(path4)\n\n    return layers.concatenate([path1, path2, path3, path4], axis=-1)\n\ndef build_googlenet_model(IMAGE_HEIGHT=224, IMAGE_WIDTH=224, NUM_CLASSES=1):\n    input_layer = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n\n    x = layers.Conv2D(64, (7,7), strides=(2,2), padding='same', activation='relu')(input_layer)\n    x = layers.MaxPooling2D((3,3), strides=(2,2), padding='same')(x)\n    x = layers.Conv2D(64, (1,1), activation='relu')(x)\n    x = layers.Conv2D(192, (3,3), padding='same', activation='relu')(x)\n    x = layers.MaxPooling2D((3,3), strides=(2,2), padding='same')(x)\n\n    # Inception modules\n    x = inception_module(x, 64, 96, 128, 16, 32, 32)\n    x = inception_module(x, 128, 128, 192, 32, 96, 64)\n    x = layers.MaxPooling2D((3,3), strides=(2,2), padding='same')(x)\n\n    x = inception_module(x, 192, 96, 208, 16, 48, 64)\n    x = inception_module(x, 160, 112, 224, 24, 64, 64)\n    x = inception_module(x, 128, 128, 256, 24, 64, 64)\n    x = inception_module(x, 112, 144, 288, 32, 64, 64)\n    x = inception_module(x, 256, 160, 320, 32, 128, 128)\n    x = layers.MaxPooling2D((3,3), strides=(2,2), padding='same')(x)\n\n    x = inception_module(x, 256, 160, 320, 32, 128, 128)\n    x = inception_module(x, 384, 192, 384, 48, 128, 128)\n\n    # Final layers\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.4)(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n\n    model = models.Model(inputs=input_layer, outputs=output)\n\n    # Compile\n    model.compile(\n        optimizer=optimizers.Adam(1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc'),\n            metrics.TruePositives(name='true_positives'),\n            metrics.TrueNegatives(name='true_negatives'),\n            metrics.FalsePositives(name='false_positives'),\n            metrics.FalseNegatives(name='false_negatives')\n        ]\n    )\n\n    model.summary()\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model8 = build_googlenet_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history8 = model8.fit(train_ds, validation_data = validation_ds,\n                    epochs=100, verbose = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Proposed Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers, losses\nimport numpy as np\nfrom tensorflow.keras import metrics\n\n# Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim                      # Total embedding dimension\n        self.num_heads = num_heads                      # Number of attention heads\n        self.head_dim = embed_dim // num_heads          # Dimension per attention head\n        \n        # Ensure embedding dimension is divisible by number of heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        # Dense layers to project inputs into queries, keys, and values\n        self.query_dense = layers.Dense(embed_dim)  \n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        \n        # Dense layer to combine multiple attention heads output\n        self.combine_heads = layers.Dense(embed_dim)\n        \n    def attention(self, query, key, value):\n        # Calculate scaled dot-product attention scores\n        score = tf.matmul(query, key, transpose_b=True)   # Compute dot product of query and key^T\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # Dimensionality for scaling\n        scaled_score = score / tf.math.sqrt(dim_key)      # Scale scores to stabilize gradients\n        weights = tf.nn.softmax(scaled_score, axis=-1)    # Apply softmax to get attention weights\n        output = tf.matmul(weights, value)                 # Weighted sum of values based on attention weights\n        return output, weights\n    \n    def separate_heads(self, x, batch_size):\n        # Reshape the input tensor to split into multiple attention heads\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim)) \n        # Transpose to shape: (batch_size, num_heads, sequence_length, head_dim)\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        \n        # Flatten spatial dimensions (height and width) into one sequence dimension\n        input_shape = tf.shape(inputs)\n        height, width = input_shape[1], input_shape[2]\n        x = tf.reshape(inputs, (batch_size, height * width, input_shape[3]))  # shape: (batch, seq_len, channels)\n        \n        # Create queries, keys, and values projections\n        query = self.query_dense(x)\n        key = self.key_dense(x)\n        value = self.value_dense(x)\n        \n        # Separate each into multiple heads\n        query = self.separate_heads(query, batch_size)\n        key = self.separate_heads(key, batch_size)\n        value = self.separate_heads(value, batch_size)\n        \n        # Compute attention output and weights\n        attention_output, attention_weights = self.attention(query, key, value)\n        \n        # Transpose back to (batch_size, seq_len, embed_dim)\n        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n        \n        # Concatenate attention heads\n        concat_attention = tf.reshape(attention_output, (batch_size, height * width, self.embed_dim))\n        \n        # Final dense layer to combine heads into output\n        output = self.combine_heads(concat_attention)\n        \n        # Reshape output back to spatial format (batch_size, height, width, embed_dim)\n        output = tf.reshape(output, (batch_size, height, width, self.embed_dim))\n        \n        return output\n\n# Spatial Attention Block\n#SpatialAttentionBlock is designed to perform spatial attention—that is, it learns where in the image the model should \"pay more attention\" by assigning weights to each spatial location\nclass SpatialAttentionBlock(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(SpatialAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        # 1x1 convolutions for query, key, and value feature generation\n        self.conv1 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv2 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv3 = layers.Conv2D(filters, 1, activation='relu')\n        self.conv_out = layers.Conv2D(filters, 1)  # Final convolution after attention\n        self.softmax = layers.Softmax(axis=-1)    # Softmax for attention weights\n        \n    def call(self, inputs):\n        # Get dynamic shapes for batch size, height, width, channels\n        batch_size, height, width, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n        \n        # Generate query, key, and value feature maps from input\n        query = self.conv1(inputs)\n        key = self.conv2(inputs)\n        value = self.conv3(inputs)\n        \n        # Reshape to (batch_size, sequence_length, filters) for matrix multiplication\n        query = tf.reshape(query, (batch_size, height * width, self.filters))\n        key = tf.reshape(key, (batch_size, height * width, self.filters))\n        value = tf.reshape(value, (batch_size, height * width, self.filters))\n        \n        # Compute attention scores by dot product of query and key transpose\n        attention_scores = tf.matmul(query, key, transpose_b=True)\n        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.filters, tf.float32))  # Scale scores\n        \n        # Softmax to obtain attention weights\n        attention_weights = self.softmax(attention_scores)\n        \n        # Weighted sum of values based on attention weights\n        attended_features = tf.matmul(attention_weights, value)\n        \n        # Reshape attended features back to spatial format\n        attended_features = tf.reshape(attended_features, (batch_size, height, width, self.filters))\n        \n        # Final convolution to refine features\n        output = self.conv_out(attended_features)\n        \n        # Add residual connection if input channels match filter count\n        if inputs.shape[-1] == self.filters:\n            output = output + inputs\n        \n        return output\n\n# Channel Attention Block\n#In Channel Attention, the model learns to weigh the importance of each channel in a feature map.\nclass ChannelAttentionBlock(layers.Layer):\n    def __init__(self, filters, reduction_ratio=16, **kwargs):\n        super(ChannelAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n        \n        # Global average and max pooling layers\n        self.global_avg_pool = layers.GlobalAveragePooling2D()\n        self.global_max_pool = layers.GlobalMaxPooling2D()\n        \n        # Dense layers to learn channel-wise attention weights\n        self.dense1 = layers.Dense(filters // reduction_ratio, activation='relu')\n        self.dense2 = layers.Dense(filters, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Average pooling branch: captures average channel information\n        avg_pool = self.global_avg_pool(inputs)                     # Shape: (batch_size, channels)\n        avg_pool = tf.expand_dims(tf.expand_dims(avg_pool, 1), 1)   # Reshape to (batch_size, 1, 1, channels)\n        avg_pool = self.dense1(avg_pool)                             # Reduce dimension and non-linearity\n        avg_pool = self.dense2(avg_pool)                             # Channel attention weights via sigmoid\n        \n        # Max pooling branch: captures prominent channel features\n        max_pool = self.global_max_pool(inputs)\n        max_pool = tf.expand_dims(tf.expand_dims(max_pool, 1), 1)\n        max_pool = self.dense1(max_pool)\n        max_pool = self.dense2(max_pool)\n        \n        # Combine attention weights from both branches\n        attention = avg_pool + max_pool\n        \n        # Multiply input features by attention weights (channel-wise scaling)\n        return inputs * attention\n\n# ResNet-like Block with Attention\nclass ResidualAttentionBlock(layers.Layer):\n    def __init__(self, filters, use_attention=True, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.use_attention = use_attention\n        \n        # Two convolutional layers with batch normalization\n        self.conv1 = layers.Conv2D(filters, 3, padding='same', activation='relu')\n        self.bn1 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n        self.bn2 = layers.BatchNormalization()\n        \n        # Attention blocks (spatial and channel) if enabled\n        if use_attention:\n            self.spatial_attention = SpatialAttentionBlock(filters)\n            self.channel_attention = ChannelAttentionBlock(filters)\n        \n        self.relu = layers.ReLU()\n        \n    def call(self, inputs):\n        # First conv layer with activation and batch norm\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        \n        # Second conv layer with batch norm (no activation yet)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        # Apply attention blocks if enabled\n        if self.use_attention:\n            x = self.spatial_attention(x)\n            x = self.channel_attention(x)\n        \n        # Residual skip connection if input and output have same channel dimension\n        if inputs.shape[-1] == self.filters:\n            x = x + inputs\n        \n        # Final activation after residual addition\n        return self.relu(x)\n\n# CNN Model with Self-Attention\ndef build_cnn_attention_model(IMAGE_WIDTH=224, IMAGE_HEIGHT=224, NUM_CLASSES=2):\n    \"\"\"\n    Build CNN model with self-attention mechanisms\n    \n    Args:\n        IMAGE_WIDTH: Input image width\n        IMAGE_HEIGHT: Input image height  \n        NUM_CLASSES: Number of output classes\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    \n    inputs = layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))  # Input layer with image shape\n    \n    # Normalize pixel values to [0,1]\n    x = layers.Rescaling(1./255)(inputs)\n    \n    # Initial convolutional layer with large kernel and stride for downsampling\n    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n    \n    # Stage 1: Basic convolutions with batch norm\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Stage 2: Residual blocks with attention + downsampling\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    x = ResidualAttentionBlock(128, use_attention=True)(x)\n    \n    # Stage 3: Increased filter count and residual attention blocks + downsampling\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    x = ResidualAttentionBlock(256, use_attention=True)(x)\n    \n    # Stage 4: High-level features + self-attention + downsampling\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Apply self-attention over feature map\n    x = SelfAttention(embed_dim=512, num_heads=8)(x)\n    \n    # Additional residual attention blocks after self-attention\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    x = ResidualAttentionBlock(512, use_attention=True)(x)\n    \n    # Global average pooling to reduce spatial dimensions\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Dropout for regularization to prevent overfitting\n    x = layers.Dropout(0.5)(x)\n    \n    # Dense layer with ReLU activation for learning complex features\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer:\n    # For binary classification, use sigmoid activation and binary crossentropy loss\n    # For multi-class, use softmax activation and sparse categorical crossentropy\n    if NUM_CLASSES == 2:\n        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n        loss_fn = losses.BinaryCrossentropy()\n    else:\n        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n        loss_fn = losses.SparseCategoricalCrossentropy()\n    \n    # Create the Keras model instance\n    model = Model(inputs, outputs, name=\"CNN_SelfAttention\")\n    \n    # Compile model with Adam optimizer, selected loss, and common classification metrics\n    optimizer = optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=[\n            'accuracy',                            # Classification accuracy\n            metrics.Precision(name='precision'), # Precision metric\n            metrics.Recall(name='recall'),       # Recall metric\n            metrics.AUC(name='auc')               # Area under ROC curve\n        ]\n    )\n\n    return model\n\n\n# Usage example\nif __name__ == \"__main__\":\n    # Define input image dimensions and number of classes\n    IMAGE_WIDTH = 224\n    IMAGE_HEIGHT = 224\n    NUM_CLASSES = 2\n    \n    print(\"Building CNN with Self-Attention model...\")\n    \n    # Build the model\n    model = build_cnn_attention_model(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CLASSES)\n    \n    # Print the model summary showing layers and parameters\n    model.summary()\n    \n    # Generate a dummy input tensor (batch size 1, 224x224 RGB image)\n    dummy_input = tf.random.normal((1, IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n    \n    # Run the model on dummy data to test forward pass\n    output = model(dummy_input)\n    \n    # Print output tensor shape\n    print(f\"\\nModel output shape: {output.shape}\")\n    \n    # Print actual output values (probabilities or logits)\n    print(f\"Output value: {output.numpy()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom torchvision import datasets\nfrom torch.utils.data import random_split\n\n# Original dataset root folder\ndata_dir = \"/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN\"\n\n# Where to save split datasets\noutput_dir = \"/kaggle/working/dataset_split\"\ntrain_dir = os.path.join(output_dir, \"train\")\ntest_dir = os.path.join(output_dir, \"test\")\n\n# Load full dataset\nfull_dataset = datasets.ImageFolder(root=data_dir)\nclass_names = full_dataset.classes\nprint(f\"Classes: {class_names}\")\n\n# Create folder structure for train/test split\nfor split_dir in [train_dir, test_dir]:\n    for cls in class_names:\n        os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n\n# Split dataset indices\ntrain_size = int(0.7 * len(full_dataset))\ntest_size = len(full_dataset) - train_size\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Function to copy images to target folder based on split\ndef save_split_dataset(dataset, split_folder):\n    for idx in dataset.indices:\n        path, label = full_dataset.samples[idx]\n        class_name = class_names[label]\n        filename = os.path.basename(path)\n        dest = os.path.join(split_folder, class_name, filename)\n        shutil.copy2(path, dest)\n\n# Save train and test splits\nsave_split_dataset(train_dataset, train_dir)\nsave_split_dataset(test_dataset, test_dir)\n\nprint(\"Datasets saved in folder structure:\")\nprint(f\"Train: {train_dir}\")\nprint(f\"Test: {test_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Paths to train and test directories\ntrain_dir = \"/kaggle/working/dataset_split/train\"\ntest_dir = \"/kaggle/working/dataset_split/test\"\n\n# Parameters\nIMAGE_SIZE = (224,224)   # Match your model input size\nBATCH_SIZE = 32\n\n# Load training dataset\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    label_mode='binary',        # since you have 2 classes: Abnormal and Normal\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# Load test dataset\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    label_mode='binary',\n    image_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras import layers, models, optimizers, losses, metrics\nfrom tensorflow.keras.applications import EfficientNetB0\n\n# Parameters\nIMAGE_SIZE = (224, 224)\nBATCH_SIZE = 32\nEPOCHS = 10\nNUM_CLASSES = 1\nDATA_DIR = '/kaggle/input/resized-dataset-ecgan/resized_dataset_ECGAN'  # Replace if needed\n\n# Step 1: Create dataframe with file paths and labels\nimage_paths = []\nlabels = []\nfor label, subdir in enumerate(sorted(os.listdir(DATA_DIR))):\n    folder_path = os.path.join(DATA_DIR, subdir)\n    for fname in os.listdir(folder_path):\n        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n            image_paths.append(os.path.join(folder_path, fname))\n            labels.append(label)\n\ndf = pd.DataFrame({'filepath': image_paths, 'label': labels})\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n\n# Define the model\ndef build_model(image_height=224, image_width=224, num_classes=1):\n    inputs = layers.Input(shape=(image_height, image_width, 3))\n    x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n    \n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=x)\n    base_model.trainable = False  # Freeze base\n    \n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4)(x)\n    outputs = layers.Dense(num_classes, activation='sigmoid')(x)  # sigmoid for binary\n\n    model = models.Model(inputs, outputs)\n\n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-3),\n        loss=losses.BinaryCrossentropy(),\n        metrics=[\n            \"accuracy\",\n            metrics.Precision(name='precision'),\n            metrics.Recall(name='recall'),\n            metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n# Step 2: Apply Stratified K-Fold\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df['filepath'], df['label'])):\n    print(f\"\\n========== Fold {fold+1} ==========\")\n\n    train_files = df.iloc[train_idx]\n    val_files = df.iloc[val_idx]\n\n    # Function to load dataset from DataFrame\n    def load_dataset(file_df):\n        paths = file_df['filepath'].values\n        labels = file_df['label'].values\n        ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n\n        def process(path, label):\n            img = tf.io.read_file(path)\n            img = tf.image.decode_jpeg(img, channels=3)\n            img = tf.image.resize(img, IMAGE_SIZE)\n            img = tf.keras.applications.efficientnet.preprocess_input(img)\n            return img, tf.cast(label, tf.float32)\n\n        ds = ds.map(process).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n        return ds\n\n    train_ds = load_dataset(train_files)\n    val_ds = load_dataset(val_files)\n\n    # Step 3: Build and train model\n    model = build_model()\n    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)\n\n    # Optional: Evaluate on validation\n    print(\"\\nValidation Metrics:\")\n    model.evaluate(val_ds, verbose=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}